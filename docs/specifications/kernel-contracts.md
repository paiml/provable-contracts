# Kernel Contracts Specification v1.0.0

**The Systematic Method for Extracting Provable Contracts from Papers**

This document defines the kernel contract registry, the paper-to-kernel
extraction methodology, and the roadmap for expanding provable coverage
across the PAIML Sovereign AI stack.

---

## Table of Contents

1. [Overview](#1-overview)
2. [Paper-to-Kernel Extraction](#2-paper-to-kernel-extraction)
3. [Kernel Equivalence Classes](#3-kernel-equivalence-classes)
4. [Contract Anatomy](#4-contract-anatomy)
5. [Existing Kernel Contracts](#5-existing-kernel-contracts)
6. [Paper Reference Registry](#6-paper-reference-registry)
7. [Binding Registry](#7-binding-registry)
8. [Gap Analysis](#8-gap-analysis)
9. [Planned Contracts](#9-planned-contracts)
10. [Integration with apr-model-qa-playbook](#10-integration-with-apr-model-qa-playbook)
11. [Contract Authoring Guide](#11-contract-authoring-guide)

---

## 1. Overview

A **kernel contract** is a YAML specification that formally connects a
peer-reviewed paper to a Rust implementation via:

1. **Equations** — the canonical math extracted from the paper
2. **Proof obligations** — typed properties that must hold
3. **Falsification tests** — proptest/probar tests designed to fail
4. **Kani harnesses** — bounded model checking proofs
5. **QA gate** — the certeza-compatible quality gate

Every kernel in the PAIML stack (aprender, trueno, realizar) that
implements a published algorithm MUST have a corresponding contract.
Uncontracted kernels are technical debt.

### Design Principles

| Principle | Source | Application |
|---|---|---|
| Falsificationism | Popper (1963) | Tests designed to refute, not confirm |
| Jidoka | Toyota (Ohno 1988) | Stop the line on first failure |
| Poka-Yoke | Toyota (Shingo 1986) | Contracts make errors impossible |
| Defense in depth | — | probar (L4) + Kani (L5) + QA gate |

### Contract Coverage Targets

| Tier | Scope | Target |
|---|---|---|
| Tier 1 | Core transformer kernels | 100% contracted |
| Tier 2 | Compound/composite kernels | 100% contracted |
| Tier 3 | Classical ML kernels | Best-effort |

---

## 2. Paper-to-Kernel Extraction

The extraction methodology converts a paper's mathematical claims into
a falsifiable contract. This is Phase 1 (Extract) and Phase 2 (Specify)
of the six-phase pipeline.

### 2.1 Identification: Finding Contractable Kernels

A kernel is contractable if it meets ALL of:

1. **Published** — appears in a peer-reviewed paper or established
   technical report
2. **Deterministic** — given the same inputs, produces the same outputs
   (modulo floating-point tolerance)
3. **Bounded** — operates on finite-dimensional inputs with known bounds
4. **Implemented** — has or will have a Rust implementation in the stack

**Sources for kernel identification:**

| Source | Method | Example |
|---|---|---|
| Paper equations | Extract from Section 3 "Method" | Softmax from Bridle (1990) |
| Reference implementations | Reverse-engineer from PyTorch/JAX | RoPE from Su et al. (2021) |
| Architecture specifications | Model config defines required ops | Qwen2 uses RMSNorm, SwiGLU, GQA |
| QA playbook failures | G4 garbage output reveals kernel bugs | LAYOUT-002 from tensor transpose |
| Performance profiling | Roofline analysis reveals hot kernels | Q4K GEMV from Williams (2009) |
| Bug reports | Five Whys traces to root kernel | PMAT-232 layer parity |

### 2.2 Extraction: Paper to Canonical Math

For each identified kernel:

**Step 1: Locate the defining equation.** Look for:
- Numbered equations in the paper (e.g., Eq. 1, Eq. 7)
- Pseudocode in algorithm blocks
- Mathematical definitions in the method section

**Step 2: Normalize to canonical form.**
- Use standard notation (no paper-specific abbreviations)
- Specify domain and codomain explicitly
- List all invariants stated or implied by the paper

**Step 3: Identify the implementation variant.**
Papers often present idealized math. The implementation may differ:

| Paper says | Implementation does | Why |
|---|---|---|
| `exp(x_i) / Σ exp(x_j)` | `exp(x_i - max(x)) / Σ exp(x_j - max(x))` | Numerical stability |
| `1/√d_k` | Pre-computed constant | Performance |
| `QK^T` | Tiled blocked matmul | Cache efficiency |
| `softmax(QK^T/√d_k)V` | Online softmax with tiling | O(N) memory |

**Step 4: Extract invariants.** Every equation implies properties:

| Equation property | Obligation type |
|---|---|
| Output sums to 1 | invariant |
| Output in range [a, b] | bound |
| `f(f(x)) = f(x)` | idempotency |
| `f(αx) = αf(x)` | linearity |
| `x ≥ y ⟹ f(x) ≥ f(y)` | monotonicity |
| SIMD matches scalar | equivalence |
| `‖f(x)‖ = ‖x‖` | conservation |
| `f(x, y) = f(y, x)` | symmetry |
| `(a ⊕ b) ⊕ c = a ⊕ (b ⊕ c)` | associativity |

### 2.3 Specification: Math to YAML

Translate the extracted math into the contract schema:

```yaml
metadata:
  version: "1.0.0"
  description: "<Kernel name> — <one-line description>"
  references:
    - "<Author> (<Year>) <Title>"

equations:
  <name>:
    formula: "<canonical math>"
    domain: "<input space>"
    codomain: "<output space>"
    invariants:
      - "<property 1>"
      - "<property 2>"

proof_obligations:
  - type: <invariant|bound|monotonicity|equivalence|...>
    property: "<human-readable name>"
    formal: "<mathematical statement>"
    tolerance: <float>
```

### 2.4 Architecture-Driven Extraction

Model architecture configs directly define required kernels.
A Qwen2 config implies these contracts are needed:

```
Qwen2Config:
  hidden_size: 896       → matmul kernel (GEMV projections)
  num_attention_heads: 14 → attention kernel (scaled dot-product)
  num_key_value_heads: 2  → GQA kernel (grouped query attention)
  intermediate_size: 4864 → activation kernel (SwiGLU = SiLU × gate)
  rms_norm_eps: 1e-6      → rmsnorm kernel
  rope_theta: 1000000     → rope kernel
  vocab_size: 151936      → embedding lookup (trivial, no contract needed)
```

This gives the **minimum set** of kernel contracts required for any
architecture. The apr-model-qa-playbook `architecture` field maps to
this kernel set.

---

## 3. Kernel Equivalence Classes

Adapted from apr-model-qa-playbook's kernel classification. Models
sharing the same architecture share the same kernel contracts.

| Class | Architectures | Required Kernels |
|---|---|---|
| **A** | Qwen2, Qwen2.5, DeepSeek | RMSNorm, RoPE, GQA, SwiGLU, MatMul |
| **B** | LLaMA 2/3, Mistral, Gemma 2 | RMSNorm, RoPE, MHA/GQA, SwiGLU, MatMul |
| **C** | BLOOM, Falcon, GPT-Neo | LayerNorm, ALiBi/Learned PE, MHA, GELU, MatMul |
| **D** | GPT-2, Phi, StableLM | LayerNorm, Learned PE, MHA, GELU/NewGELU, MatMul |
| **E** | Mamba, RWKV | SSM kernels, no attention |
| **F** | Whisper, Moonshine | Encoder-decoder attention, Conv1d, Mel spectrogram |

**Implication:** Proving softmax correct for Class A covers Qwen2,
Qwen2.5, and DeepSeek simultaneously. This is the kernel proof
reference (`kernel_proof_ref`) concept from the QA playbook.

---

## 4. Contract Anatomy

Every kernel contract follows this structure:

```
┌─────────────────────────────────────────┐
│ metadata                                │ ← Paper references, version
├─────────────────────────────────────────┤
│ equations                               │ ← Canonical math from paper
│   formula, domain, codomain, invariants │
├─────────────────────────────────────────┤
│ proof_obligations                       │ ← Typed properties to verify
│   type, property, formal, tolerance     │
├─────────────────────────────────────────┤
│ kernel_structure (optional)             │ ← Implementation phases
│   phases[], simd_dispatch, enforcement  │
├─────────────────────────────────────────┤
│ falsification_tests                     │ ← probar/proptest tests
│   id, prediction, if_fails             │
├─────────────────────────────────────────┤
│ kani_harnesses                          │ ← Bounded model checking
│   id, obligation, strategy, bound       │
├─────────────────────────────────────────┤
│ qa_gate                                 │ ← certeza quality gate
│   id, checks, pass_criteria            │
└─────────────────────────────────────────┘
```

### Obligation Types

| Type | Pattern | Example |
|---|---|---|
| `invariant` | `∀x: P(f(x))` | Softmax sums to 1 |
| `bound` | `∀x: a ≤ f(x) ≤ b` | Softmax output in (0, 1) |
| `monotonicity` | `x ≥ y ⟹ f(x) ≥ f(y)` | ReLU monotonic |
| `equivalence` | `\|f_simd(x) - f_scalar(x)\| < ε` | SIMD parity |
| `idempotency` | `f(f(x)) = f(x)` | RMSNorm re-normalization |
| `linearity` | `f(αx + βy) = αf(x) + βf(y)` | MatMul distributivity |
| `symmetry` | `f(x, y) = f(y, x)` | Dot product commutativity |
| `associativity` | `(a ⊕ b) ⊕ c = a ⊕ (b ⊕ c)` | MatMul associativity |
| `conservation` | `Q(before) = Q(after)` | RoPE norm preservation |

### Verification Strategies

| Strategy | When to use | Kani capability |
|---|---|---|
| `exhaustive` | Small integer domains | Full state space |
| `stub_float` | Floating-point kernels | Stub transcendentals |
| `compositional` | Compound kernels | Verify per-component |

---

## 5. Existing Kernel Contracts

### 5.1 Core Transformer Kernels (13 contracts)

#### Softmax (`softmax-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Bridle (1990), Milakov & Gimelshein (2018) |
| Equation | `σ(x)_i = exp(x_i - max(x)) / Σ_j exp(x_j - max(x))` |
| Obligations | 6 (3 invariant, 1 bound, 1 monotonicity, 1 equivalence) |
| Falsification | 6 tests (normalization, positivity, order, SIMD, boundaries) |
| Kani | 3 harnesses (normalization, positivity, bounded) |
| Key invariant | `Σ σ(x)_i = 1.0` (normalization to probability simplex) |
| SIMD tolerance | 8 ULP |
| Kernel phases | find_max → exp_subtract → sum_exp → normalize |

#### RMSNorm (`rmsnorm-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Zhang & Sennrich (2019), Touvron et al. (2023) |
| Equation | `RMSNorm(x)_i = (x_i / RMS(x)) · γ_i` |
| Obligations | 5 (2 invariant, 1 bound, 1 equivalence, 1 idempotency) |
| Falsification | 5 tests (finiteness, scale invariance, SIMD, zero, unit-γ) |
| Kani | 2 harnesses (finiteness, RMS positive) |
| Key invariant | Scale invariance: `RMSNorm(αx) = sign(α)·RMSNorm(x)` |
| SIMD tolerance | 4 ULP |
| Kernel phases | sum_squares → compute_rms → normalize_scale |

#### RoPE (`rope-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Su et al. (2021) |
| Equation | 2D rotation per pair: `cos(mθ_k)` / `sin(mθ_k)` |
| Obligations | 4 (1 conservation, 1 invariant, 1 equivalence, 1 bound) |
| Falsification | 4 tests (norm preservation, relative position, SIMD, identity) |
| Kani | 1 harness (norm preservation) |
| Key invariant | `‖RoPE(x, m)‖ = ‖x‖` (norm preservation / isometry) |
| SIMD tolerance | 4 ULP |
| Kernel phases | compute_freqs → compute_sincos → rotate_pairs |

#### Activation (`activation-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Hendrycks & Gimpel (2016), Ramachandran (2017), Nair & Hinton (2010) |
| Equations | GELU, SiLU/Swish, ReLU (3 equations) |
| Obligations | 6 (3 invariant, 1 bound, 1 monotonicity, 1 equivalence) |
| Falsification | 5 tests (GELU zero, GELU approx, SiLU zero, ReLU, SIMD) |
| Kani | 2 harnesses (ReLU non-negative, ReLU monotonic) |
| Key invariant | `ReLU(x) ≥ 0` (non-negativity, exhaustively provable) |
| SIMD tolerance | 4 ULP |

#### Attention (`attention-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Vaswani et al. (2017) |
| Equation | `Attention(Q,K,V) = softmax(QK^T/√d_k)·V` |
| Obligations | 5 (2 invariant, 1 bound, 1 equivalence, 1 invariant) |
| Falsification | 4 tests (weight normalization, convexity, scaling, SIMD) |
| Kani | 1 harness (weight normalization) |
| Key invariant | Each output row is a convex combination of V rows |
| SIMD tolerance | 8 ULP |
| Kernel phases | compute_scores → scale → softmax_rows → weighted_sum |

#### MatMul (`matmul-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Goto & van de Geijn (2008), Dettmers et al. (2022) |
| Equations | Standard matmul + quantized dot product (2 equations) |
| Obligations | 5 (1 invariant, 1 associativity, 1 linearity, 1 equiv, 1 bound) |
| Falsification | 5 tests (shape, accuracy, SIMD, quantized, identity) |
| Kani | 1 harness (quantized dot bounded) |
| Key invariant | `shape(A @ B) = (rows(A), cols(B))` |
| SIMD tolerance | 4 ULP |
| Kernel phases | tile_partition → micro_kernel → store_result |

#### Flash Attention (`flash-attention-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Dao et al. (2022), Dao (2023) |
| Equation | Standard attention computed in O(N) memory via tiling |
| Obligations | 4 (1 equivalence, 1 invariant, 1 invariant, 1 conservation) |
| Falsification | 4 tests (equivalence, online softmax, normalization, single tile) |
| Kani | 1 harness (online softmax 2 tiles) |
| Key invariant | Exact equivalence to standard attention (not approximate) |
| Kernel phases | outer_loop → inner_loop → online_softmax → accumulate |

#### SwiGLU (`swiglu-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Shazeer (2020), Ramachandran et al. (2017) |
| Equations | SwiGLU, SiLU (2 equations) |
| Obligations | 4 (1 invariant, 1 bound, 2 equivalence) |
| Falsification | 6 tests (zero preservation, fused equiv, SiLU bound, SIMD, boundary, gate monotonicity) |
| Kani | 3 harnesses (zero preservation, fused equivalence, SiLU bound) |
| Key invariant | `SwiGLU(0, W, V, 0, 0) = 0` (zero preservation) |
| SIMD tolerance | 8 ULP |
| Kernel phases | linear_gate → linear_value → silu_activation → elementwise_multiply |

#### GQA (`gqa-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Ainslie et al. (2023), Vaswani et al. (2017) |
| Equation | `GQA(Q, K, V) = softmax(Q_g * K_h^T / sqrt(d_k)) * V_h` |
| Obligations | 5 (2 invariant, 1 bound, 2 equivalence) |
| Falsification | 6 tests (weight normalization, MHA degeneration, convex bound, head divisibility, SIMD, MQA boundary) |
| Kani | 3 harnesses (weight normalization, MHA equivalence, convex bound) |
| Key invariant | `GQA(kv_heads=num_heads) = MHA(Q, K, V)` (degeneration to standard MHA) |
| SIMD tolerance | 8 ULP |
| Kernel phases | kv_broadcast → qk_matmul → attention_softmax → weighted_sum |

#### LayerNorm (`layernorm-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Ba et al. (2016), Ioffe & Szegedy (2015) |
| Equations | LayerNorm, statistics (2 equations) |
| Obligations | 6 (3 invariant, 1 bound, 1 equivalence, 1 idempotency) |
| Falsification | 7 tests (centering, standardization, denominator, SIMD, idempotency, shift invariance, constant input) |
| Kani | 3 harnesses (centering, standardization, denominator positive) |
| Key invariant | `var(LN(x)) ≈ 1` when γ=1, β=0 (standardization) |
| SIMD tolerance | 8 ULP |
| Kernel phases | compute_mean → compute_variance → normalize → affine_transform |

#### SiLU (`silu-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Ramachandran et al. (2017), Elfwing et al. (2018) |
| Equations | SiLU, sigmoid (2 equations) |
| Obligations | 5 (1 invariant, 2 bound, 1 monotonicity, 1 equivalence) |
| Falsification | 6 tests (zero preservation, lower bound, positive monotonicity, SIMD, asymptotic linearity, large negative) |
| Kani | 3 harnesses (zero, lower bound, positive monotonicity) |
| Key invariant | `SiLU(0) = 0` and `SiLU(x) > -0.279` (zero preservation, global minimum) |
| SIMD tolerance | 8 ULP |
| Kernel phases | compute_sigmoid → multiply |

#### Cross-Entropy (`cross-entropy-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Shannon (1948), Milakov & Gimelshein (2018) |
| Equations | cross_entropy, log_softmax (2 equations) |
| Obligations | 5 (1 invariant, 2 bound, 2 equivalence) |
| Falsification | 6 tests (non-negativity, log-softmax bound, numerical stability, decomposition equiv, SIMD, perfect prediction) |
| Kani | 3 harnesses (non-negative, log-softmax bound, finite output) |
| Key invariant | `CE(targets, logits) >= 0` (non-negativity) |
| SIMD tolerance | 8 ULP |
| Kernel phases | find_max → log_sum_exp → log_softmax → nll |

#### AdamW (`adamw-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Loshchilov & Hutter (2017), Kingma & Ba (2014) |
| Equations | adam_moments, adam_variance, bias_correction, weight_update (4 equations) |
| Obligations | 5 (2 invariant, 2 bound, 1 equivalence) |
| Falsification | 6 tests (decoupled decay, moment non-negativity, bias correction, update finiteness, SIMD, zero gradient) |
| Kani | 3 harnesses (decoupled, moment positive, finite update) |
| Key invariant | Weight decay applied AFTER Adam update (decoupled) |
| SIMD tolerance | 8 ULP |
| Kernel phases | update_first_moment → update_second_moment → bias_correct → adam_step → weight_decay |

### 5.2 Aprender-Specific Contracts (3 contracts)

#### Layer Parity (`contracts/aprender/layer-parity-v1.yaml`)

Defines the 14-step transformer layer forward pass and requires all 4
backends (CPU SIMD, GPU workspace, GPU graphed, GPU indexed) to produce
equivalent results within per-step tolerances.

| Step | Operation | Tolerance (abs) |
|---|---|---|
| attn_norm | RMSNorm | 1e-4 |
| q/k/v_proj | Quantized GEMV | 1e-3 |
| rope | Rotary embedding | 1e-4 |
| attention | Scaled dot-product | 1e-3 |
| o_proj | Output projection | 1e-3 |
| residual1 | Skip connection | exact |
| ffn_norm | RMSNorm | 1e-4 |
| gate/up_proj | FFN projections | 1e-3 |
| swiglu | Fused activation | 1e-4 |
| down_proj | FFN output | 1e-3 |
| residual2 | Skip connection | exact |

**Root cause:** PMAT-232 (7B GPU garbage output). Five Whys traced to
missing `LayerForward` abstraction across backends.

#### Kernel Fusion (`contracts/aprender/kernel-fusion-v1.yaml`)

Documents every GPU kernel fusion decision with Toyota Poka-Yoke
enforcement. Each fusion entry records:

- Fused kernel and what it replaces
- Saves per layer and per sequence
- Memory savings in bytes
- Benchmark data
- Falsification: "replacing fused with unfused must be slower"

**Active fusions:** SwiGLU (FUSION-001), Batched SwiGLU (FUSION-002).
**Blocked fusions:** RMSNorm+Gate+Up+SwiGLU (PAR-077: 3x slower).

#### Binding Registry (`contracts/aprender/binding.yaml`)

Maps each contract equation to the aprender function that implements it:

| Contract | Equation | aprender Path | Status |
|---|---|---|---|
| softmax-kernel-v1 | softmax | `nn::functional::softmax` | Implemented |
| rmsnorm-kernel-v1 | rmsnorm | `nn::RMSNorm::forward` | Implemented |
| rope-kernel-v1 | rope | `nn::RotaryPositionEmbedding::apply` | Implemented |
| activation-kernel-v1 | gelu | `nn::functional::gelu` | Implemented |
| activation-kernel-v1 | relu | `nn::functional::relu` | Implemented |
| activation-kernel-v1 | silu | — | Not implemented |
| attention-kernel-v1 | attention | `nn::transformer::scaled_dot_product_attention` | Partial |
| matmul-kernel-v1 | matmul | `autograd::Tensor::matmul` | Implemented |
| matmul-kernel-v1 | quantized_dot | — | Not implemented |
| flash-attention-v1 | flash_attention | — | Not implemented |
| swiglu-kernel-v1 | swiglu | `models::qwen2::swiglu` | Partial |
| swiglu-kernel-v1 | silu_gate | `nn::functional::silu` | Partial |
| gqa-kernel-v1 | gqa | `nn::transformer::grouped_query_attention` | Partial |
| gqa-kernel-v1 | kv_broadcast | `nn::transformer::kv_head_broadcast` | Partial |
| layernorm-kernel-v1 | layernorm | `nn::LayerNorm::forward` | Implemented |
| layernorm-kernel-v1 | statistics | `nn::LayerNorm::compute_stats` | Implemented |
| silu-kernel-v1 | silu | — | Not implemented |
| silu-kernel-v1 | sigmoid | — | Not implemented |
| cross-entropy-kernel-v1 | cross_entropy | `nn::CrossEntropyLoss::forward` | Implemented |
| cross-entropy-kernel-v1 | log_softmax | `nn::functional::log_softmax` | Implemented |
| adamw-kernel-v1 | adam_moments | `nn::optim::AdamW::step` | Implemented |
| adamw-kernel-v1 | bias_correction | `nn::optim::AdamW::step` | Implemented |
| adamw-kernel-v1 | adam_variance | `nn::optim::AdamW::step` | Implemented |
| adamw-kernel-v1 | weight_update | `nn::optim::AdamW::step` | Implemented |

**Coverage:** 13/24 equations implemented, 9/24 fully bound.

### 5.3 Tier 3 Kernels (7 contracts)

#### SSM (`ssm-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Gu & Dao (2023), Gu et al. (2021) |
| Equations | ssm_discretize, ssm_scan, selective_gate (3 equations) |
| Obligations | 5 (2 invariant, 1 bound, 2 equivalence) |
| Falsification | 6 tests (causality, softplus positivity, scan linearity, parallel-seq equiv, SIMD, zero input) |
| Kani | 2 harnesses (causality, softplus positive) |
| Key invariant | Causality: `y_t` depends only on `x_1..x_t` |
| Kernel phases | selective_projection → discretize → parallel_scan → output_projection |

#### Conv1d (`conv1d-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | LeCun et al. (1998), Radford et al. (2023) |
| Equation | `y[n] = Σ w[k] * x[n*stride + k - pad] + bias` |
| Obligations | 5 (1 invariant, 1 linearity, 2 equivalence, 1 bound) |
| Falsification | 6 tests (output shape, linearity, im2col equiv, SIMD, kernel=1, identity kernel) |
| Kani | 2 harnesses (output shape, linearity) |
| Key invariant | `L_out = floor((L + 2*pad - K) / stride) + 1` |
| Kernel phases | im2col → gemm → add_bias |

#### BatchNorm (`batchnorm-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Ioffe & Szegedy (2015) |
| Equations | batchnorm_train, running_stats, batchnorm_eval (3 equations) |
| Obligations | 5 (2 invariant, 1 bound, 2 equivalence) |
| Falsification | 6 tests (standardization, denominator, running stats, eval mode, SIMD, batch=1) |
| Kani | 2 harnesses (denominator positive, running variance nonneg) |
| Key invariant | Eval mode uses running stats, not batch stats |
| Kernel phases | compute_batch_stats → normalize → affine_transform → update_running_stats |

#### K-Means (`kmeans-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Lloyd (1982), Arthur & Vassilvitskii (2007) |
| Equations | assignment, update, objective (3 equations) |
| Obligations | 5 (2 invariant, 1 bound, 1 monotonicity, 1 equivalence) |
| Falsification | 6 tests (nearest assignment, monotone convergence, non-negativity, valid indices, SIMD, K=1) |
| Kani | 2 harnesses (nearest centroid, objective nonneg) |
| Key invariant | `J_{t+1} ≤ J_t` (monotone convergence) |
| Kernel phases | initialize → assign → update → check_convergence |

#### PageRank (`pagerank-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Brin & Page (1998) |
| Equations | pagerank, power_iteration (2 equations) |
| Obligations | 5 (2 invariant, 1 monotonicity, 1 bound, 1 equivalence) |
| Falsification | 6 tests (probability distribution, convergence, non-negativity, SIMD, single node, uniform graph) |
| Kani | 2 harnesses (distribution sums to 1, non-negativity) |
| Key invariant | `sum(r) = 1` and `r_i ≥ 0` (valid probability distribution) |
| Kernel phases | build_transition → initialize → iterate → check_convergence |

#### L-BFGS (`lbfgs-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Nocedal (1980), Liu & Nocedal (1989) |
| Equations | two_loop_recursion, secant_condition, line_search (3 equations) |
| Obligations | 5 (2 invariant, 1 bound, 1 monotonicity, 1 equivalence) |
| Falsification | 6 tests (descent direction, curvature condition, history bound, objective decrease, SIMD, first iteration) |
| Kani | 2 harnesses (descent direction, history bound) |
| Key invariant | `g^T * direction < 0` (descent direction guarantee) |
| Kernel phases | two_loop_backward → initial_scaling → two_loop_forward → line_search |

#### CMA-ES (`cma-es-kernel-v1.yaml`)

| Field | Value |
|---|---|
| Papers | Hansen (2016), Hansen & Ostermeier (2001) |
| Equations | sample, mean_update, covariance_update (3 equations) |
| Obligations | 5 (2 invariant, 1 bound, 1 equivalence) |
| Falsification | 6 tests (step size, covariance PD, weights, symmetry, SIMD, d=1) |
| Kani | 2 harnesses (sigma positive, weights normalized) |
| Key invariant | Covariance matrix `C` remains symmetric positive definite |
| Kernel phases | sample_population → evaluate_sort → update_mean → update_paths → update_covariance → update_step_size |

---

## 6. Paper Reference Registry

All papers referenced by existing and planned kernel contracts.

### 6.1 Core Transformer Papers

| ID | Paper | Year | Kernels |
|---|---|---|---|
| P-001 | Vaswani et al. "Attention Is All You Need" | 2017 | attention, softmax |
| P-002 | Su et al. "RoFormer: Enhanced Transformer with Rotary Position Embedding" | 2021 | rope |
| P-003 | Zhang & Sennrich "Root Mean Square Layer Normalization" | 2019 | rmsnorm |
| P-004 | Touvron et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models" | 2023 | rmsnorm, rope, swiglu |
| P-005 | Dao et al. "FlashAttention: Fast and Memory-Efficient Exact Attention" | 2022 | flash_attention |
| P-006 | Dao "FlashAttention-2: Faster Attention with Better Parallelism" | 2023 | flash_attention |
| P-007 | Ainslie et al. "GQA: Training Generalized Multi-Query Transformer" | 2023 | gqa |
| P-008 | Press et al. "ALiBi: Train Short, Test Long" | 2022 | alibi |
| P-009 | Ba et al. "Layer Normalization" | 2016 | layernorm |

### 6.2 Activation Function Papers

| ID | Paper | Year | Kernels |
|---|---|---|---|
| P-010 | Hendrycks & Gimpel "Gaussian Error Linear Units (GELUs)" | 2016 | gelu |
| P-011 | Ramachandran et al. "Searching for Activation Functions" | 2017 | silu/swish |
| P-012 | Nair & Hinton "Rectified Linear Units Improve RBMs" | 2010 | relu |
| P-013 | Shazeer "GLU Variants Improve Transformer" | 2020 | swiglu, geglu |
| P-014 | Elfwing et al. "Sigmoid-Weighted Linear Units for NNs" | 2018 | silu |

### 6.3 Linear Algebra Papers

| ID | Paper | Year | Kernels |
|---|---|---|---|
| P-020 | Goto & van de Geijn "Anatomy of High-Performance MatMul" | 2008 | matmul |
| P-021 | Dettmers et al. "LLM.int8(): 8-bit Matrix Multiplication" | 2022 | quantized_dot |
| P-022 | Bridle "Training Stochastic Model Recognition Algorithms" | 1990 | softmax |
| P-023 | Milakov & Gimelshein "Online normalizer calculation for softmax" | 2018 | softmax |
| P-024 | Williams et al. "Roofline: An Insightful Visual Performance Model" | 2009 | (profiling) |

### 6.4 Quantization Papers

| ID | Paper | Year | Kernels |
|---|---|---|---|
| P-030 | Frantar et al. "GPTQ: Accurate Post-Training Quantization" | 2022 | quant_gemv |
| P-031 | Dettmers et al. "LLM.int8()" | 2022 | int8_matmul |
| P-032 | Gerganov "GGUF Format Specification" | 2023 | q4k_dequant |

### 6.5 Optimization Papers

| ID | Paper | Year | Kernels |
|---|---|---|---|
| P-040 | Kingma & Ba "Adam: A Method for Stochastic Optimization" | 2014 | adam |
| P-041 | Loshchilov & Hutter "Decoupled Weight Decay Regularization" | 2017 | adamw |
| P-042 | Boyd et al. "Distributed Optimization and Statistical Learning via ADMM" | 2011 | admm |

### 6.6 Classical ML Papers

| ID | Paper | Year | Kernels |
|---|---|---|---|
| P-050 | Brin & Page "The Anatomy of a Large-Scale Hypertextual Web Search Engine" | 1998 | pagerank |
| P-051 | Lloyd "Least Squares Quantization in PCM" | 1982 | kmeans |
| P-052 | Storn & Price "Differential Evolution" | 1997 | de_optimize |
| P-053 | Kennedy & Eberhart "Particle Swarm Optimization" | 1995 | pso |
| P-054 | Hansen "The CMA Evolution Strategy: A Tutorial" | 2016 | cma_es |

---

## 7. Binding Registry

The binding registry (`contracts/aprender/binding.yaml`) maps contract
equations to implementation functions. This enables:

- `pv audit --binding` — gap analysis
- `pv probar --binding` — wired property tests calling real code

### Binding Status Definitions

| Status | Meaning |
|---|---|
| `implemented` | Public function matches contract semantics |
| `partial` | Function exists but doesn't cover all obligations |
| `not_implemented` | No public function available |

### Wired Test Generation

When a binding has status `implemented`, `pv probar --binding` generates
tests that call the real function:

```rust
// Generated for softmax with binding to aprender::nn::functional::softmax
proptest! {
    #[test]
    fn prop_output_sums_to_1(
        data in proptest::collection::vec(-100.0f32..100.0, 1..64usize)
    ) {
        let x = Tensor::new(&data, &[1, n]);
        let y = softmax(&x, -1);
        let sum: f32 = y.data().iter().sum();
        prop_assert!((sum - 1.0).abs() < 1e-6);
    }
}
```

Without a binding, tests generate `unimplemented!()` stubs.

---

## 8. Gap Analysis

### 8.1 aprender Implementations Without Contracts

These aprender functions have implementations but no provable contract.

#### High Priority (Tier 1 — core transformer path)

**Now contracted:** SiLU (`silu-kernel-v1.yaml`), SwiGLU (`swiglu-kernel-v1.yaml`),
LayerNorm (`layernorm-kernel-v1.yaml`), GroupedQueryAttention (`gqa-kernel-v1.yaml`).

| Function | Module | Paper | Why critical |
|---|---|---|---|
| `Embedding lookup` | `nn::Embedding` | — | Trivial but high-frequency |
| `Linear` (forward) | `nn::Linear` | — | `y = Wx + b`, every projection |
| `Conv1d` | `nn::Conv1d` | — | Whisper encoder |

#### Medium Priority (Tier 2 — numerical correctness)

**Now contracted:** AdamW (`adamw-kernel-v1.yaml`), CrossEntropyLoss (`cross-entropy-kernel-v1.yaml`).

| Function | Module | Paper | Why important |
|---|---|---|---|
| `Dropout` | `nn::Dropout` | Srivastava et al. (2014) | Correct scaling at eval time |
| `im2col + GEMM` | `nn::Conv2d` | — | Convolution correctness |

#### Lower Priority (Tier 3 — classical ML)

**Now contracted:** K-Means (`kmeans-kernel-v1.yaml`), L-BFGS (`lbfgs-kernel-v1.yaml`),
CMA-ES (`cma-es-kernel-v1.yaml`), BatchNorm (`batchnorm-kernel-v1.yaml`),
PageRank (`pagerank-kernel-v1.yaml`), SSM (`ssm-kernel-v1.yaml`), Conv1d (`conv1d-kernel-v1.yaml`).

| Function | Module | Paper | Notes |
|---|---|---|---|
| Cholesky solve | `linear_model` | — | Positive-definiteness check |
| ARIMA | `time_series::arima` | Box & Jenkins (1976) | Stationarity |
| Differential Evolution | `metaheuristics::de` | Storn & Price (1997) | Population bounds |

### 8.2 Coverage by Architecture Class

| Class | Architectures | Contracted | Gap |
|---|---|---|---|
| A (Qwen2) | softmax, rmsnorm, rope, matmul, attention, swiglu, gqa | 7/7 | — |
| B (LLaMA) | same as A | 7/7 | — |
| C (BLOOM) | softmax, matmul, attention, layernorm | 4/5 | GELU (ALiBi trivial) |
| D (GPT-2) | softmax, matmul, attention, layernorm | 4/5 | GELU |
| E (Mamba) | matmul, ssm, selective_gate | 3/4 | conv1d |
| F (Whisper) | softmax, matmul, attention, conv1d | 4/6 | Mel, encoder-decoder attn |

### 8.3 apr-model-qa-playbook Gateway Coverage

The QA playbook's gateways map to kernel contracts:

| Gateway | What it detects | Kernel contracts that prevent it |
|---|---|---|
| G0 (Integrity) | Config mismatch | tensor-layout-v1 |
| G1 (Load) | Load failure | — (infrastructure, not kernel) |
| G2 (Inference) | Forward pass crash | layer-parity-v1 |
| G3 (Stability) | SIGSEGV/SIGILL | All equivalence obligations |
| G4 (Quality) | Garbage output | softmax, rmsnorm, attention, matmul |

**G4 is the primary kernel contract consumer.** When G4 detects garbage
output, the root cause is almost always a kernel bug:
- Softmax collapse → repetitive tokens
- RMSNorm NaN → all-zero or NaN outputs
- MatMul transpose → gibberish (LAYOUT-002)
- Quantization error → subtle degradation

---

## 9. Planned Contracts

All planned contracts have been implemented. See Section 5.1 (Tier 1/2) and Section 5.3 (Tier 3) for details.

### 9.1 Immediate (close gap for Class A/B architectures) — COMPLETED

All three Tier 1 contracts implemented: `swiglu-kernel-v1.yaml`, `gqa-kernel-v1.yaml`, `layernorm-kernel-v1.yaml`.

#### `swiglu-kernel-v1.yaml`

```
Paper:  Shazeer (2020) "GLU Variants Improve Transformer"
Math:   SwiGLU(x, W, V, b, c) = SiLU(xW + b) ⊙ (xV + c)
        where SiLU(x) = x · σ(x)
Key:    Composition of SiLU and element-wise multiply
Obligations:
  - invariant: SwiGLU(0, W, V, 0, 0) = 0
  - equivalence: fused SwiGLU matches unfused SiLU + multiply
  - bound: |SwiGLU(x)| bounded by input magnitude
  - equivalence: SIMD matches scalar
```

#### `gqa-kernel-v1.yaml`

```
Paper:  Ainslie et al. (2023) "GQA: Training Generalized MQT Models"
Math:   Same as MHA but with num_kv_heads < num_heads
        Each KV head shared across (num_heads / num_kv_heads) Q heads
Key:    KV head broadcasting correctness
Obligations:
  - invariant: attention weight normalization (per query head)
  - equivalence: GQA with kv_heads=num_heads equals standard MHA
  - bound: output is convex combination of V
  - equivalence: SIMD matches scalar
```

#### `layernorm-kernel-v1.yaml`

```
Paper:  Ba et al. (2016) "Layer Normalization"
Math:   LN(x)_i = γ_i · (x_i - μ) / √(σ² + ε) + β_i
        where μ = mean(x), σ² = var(x)
Key:    Centering (mean=0) + scaling (var=1) then affine transform
Obligations:
  - invariant: mean(LN(x)) ≈ β when γ=1 (centering)
  - invariant: var(LN(x)) ≈ 1 when γ=1, β=0 (standardization)
  - bound: denominator > 0 when ε > 0
  - equivalence: SIMD matches scalar
  - idempotency: LN(LN(x)) ≈ LN(x) when γ=1, β=0
```

### 9.2 Near-term (Tier 2 compound kernels) — COMPLETED

All three Tier 2 contracts implemented: `silu-kernel-v1.yaml`, `cross-entropy-kernel-v1.yaml`, `adamw-kernel-v1.yaml`.

#### `silu-kernel-v1.yaml`

```
Paper:  Ramachandran et al. (2017), Elfwing et al. (2018)
Math:   SiLU(x) = x · σ(x) = x / (1 + exp(-x))
Obligations:
  - invariant: SiLU(0) = 0
  - bound: SiLU(x) > -0.279 (global minimum)
  - monotonicity: SiLU is monotonic for x > 0
  - equivalence: SIMD matches scalar
```

#### `cross-entropy-kernel-v1.yaml`

```
Paper:  (standard; Shannon 1948 for information theory)
Math:   CE(p, q) = -Σ p_i · log(q_i)
        Implemented as: log_softmax(logits) then NLL
Key:    log-sum-exp trick for numerical stability
Obligations:
  - invariant: CE ≥ 0 (non-negativity)
  - invariant: CE(p, p) = H(p) (entropy)
  - bound: finite output for finite inputs
  - equivalence: LogSoftmax + NLL = CrossEntropy
```

#### `adamw-kernel-v1.yaml`

```
Paper:  Loshchilov & Hutter (2017) "Decoupled Weight Decay"
Math:   m_t = β₁·m_{t-1} + (1-β₁)·g_t
        v_t = β₂·v_{t-1} + (1-β₂)·g_t²
        θ_t = θ_{t-1} - lr·(m̂_t/(√v̂_t + ε) + λ·θ_{t-1})
Obligations:
  - invariant: weight decay applied AFTER Adam update (decoupled)
  - bound: bias-corrected moments finite when inputs finite
  - monotonicity: loss decreases (on convex problems, in expectation)
```

### 9.3 Future (Tier 3 — classical ML and special kernels) — COMPLETED

All seven Tier 3 contracts implemented: `ssm-kernel-v1.yaml`, `conv1d-kernel-v1.yaml`,
`batchnorm-kernel-v1.yaml`, `kmeans-kernel-v1.yaml`, `pagerank-kernel-v1.yaml`,
`lbfgs-kernel-v1.yaml`, `cma-es-kernel-v1.yaml`.

| Contract | Paper | Status |
|---|---|---|
| `kmeans-kernel-v1.yaml` | Lloyd (1982) | Implemented |
| `pagerank-kernel-v1.yaml` | Brin & Page (1998) | Implemented |
| `lbfgs-kernel-v1.yaml` | Nocedal (1980) | Implemented |
| `cma-es-kernel-v1.yaml` | Hansen (2016) | Implemented |
| `ssm-kernel-v1.yaml` | Gu & Dao (2023) Mamba | Implemented |
| `conv1d-kernel-v1.yaml` | LeCun et al. (1998) | Implemented |
| `batchnorm-kernel-v1.yaml` | Ioffe & Szegedy (2015) | Implemented |

---

## 10. Integration with apr-model-qa-playbook

### 10.1 Gateway ↔ Contract Mapping

The QA playbook gateways are **consumers** of kernel contract proofs.
When all kernel contracts for an architecture class are proven, the
corresponding gateway is structurally guaranteed to pass.

```
Kernel Contracts (provable-contracts)
  ↓ prove
Kernel Implementations (aprender/trueno)
  ↓ compose
Layer Forward (layer-parity-v1)
  ↓ execute
QA Playbook Gateways (apr-model-qa-playbook)
  ↓ certify
Model Certification (CERTIFIED/PROVISIONAL)
```

### 10.2 Falsification Gate Cross-References

| QA Falsification Gate | Kernel Contract |
|---|---|
| F-NUM-001 (attention entropy) | attention-kernel-v1 (weight normalization) |
| F-NUM-002 (LayerNorm drift) | rmsnorm-kernel-v1 / layernorm-kernel-v1 |
| F-NUM-003 (softmax sum) | softmax-kernel-v1 (normalization invariant) |
| F-PAR-001 (CPU/GPU parity) | layer-parity-v1 (all 14 steps) |
| F-PAR-002 (format parity) | tensor-layout-v1 (LAYOUT-002) |
| F-INT-001 (memory safety) | All contracts (Kani harnesses) |

### 10.3 Kernel Proof References

The QA playbook's `kernel_proof_ref` field points to a certified model.
Once kernel contracts are proven for a model's architecture class,
`kernel_proof_ref` becomes a **structural guarantee** rather than an
empirical observation:

```yaml
# Before: empirical (runs tests, hopes they pass)
metadata:
  kernel_proof_ref: "Qwen/Qwen2.5-Coder-0.5B-Instruct"

# After: structural (contracts prove correctness)
metadata:
  kernel_proof_ref: "Qwen/Qwen2.5-Coder-0.5B-Instruct"
  kernel_contracts:
    - softmax-kernel-v1
    - rmsnorm-kernel-v1
    - rope-kernel-v1
    - activation-kernel-v1
    - attention-kernel-v1
    - matmul-kernel-v1
  kernel_class: A
  proof_status: proven  # All contracts have passing Kani harnesses
```

---

## 11. Contract Authoring Guide

### 11.1 Checklist for New Contracts

1. **Identify the paper.** Find the original publication that defines
   the algorithm. If the kernel is a composition (e.g., SwiGLU = SiLU
   × gate), cite all component papers.

2. **Extract the equation.** Write the canonical mathematical form.
   Normalize notation. Specify domain and codomain.

3. **List invariants.** What properties does the paper claim or imply?
   Consider: range bounds, conservation laws, symmetries, special
   cases (zero input, identity element).

4. **Choose obligation types.** Map each invariant to a proof
   obligation type. Every contract MUST have at least one `equivalence`
   obligation (SIMD parity).

5. **Set tolerances.** Use:
   - 1e-6 for exact properties (normalization, non-negativity)
   - 1e-5 for algebraic properties (associativity, idempotency)
   - 1e-4 for approximation bounds (GELU approx, quantization)
   - 4-8 ULP for SIMD equivalence

6. **Write falsification tests.** Each obligation needs at least one
   test. Include boundary cases. Add an `if_fails` diagnosis.

7. **Add Kani harnesses.** At least one harness per contract. Choose:
   - `exhaustive` for integer/boolean domains
   - `stub_float` for floating-point domains
   - `compositional` for compound kernels

8. **Define the QA gate.** Include a `falsification_mutation` that
   describes what code change would break the contract.

9. **Add to binding.yaml.** Map the equation to the implementation
   function, or mark as `not_implemented`.

10. **Run the pipeline.**
    ```bash
    pv validate contracts/new-kernel-v1.yaml
    pv probar contracts/new-kernel-v1.yaml --binding contracts/aprender/binding.yaml
    pv audit contracts/new-kernel-v1.yaml --binding contracts/aprender/binding.yaml
    ```

### 11.2 Naming Conventions

| Element | Convention | Example |
|---|---|---|
| Contract file | `<kernel>-kernel-v<N>.yaml` | `softmax-kernel-v1.yaml` |
| Falsification ID | `FALSIFY-<XX>-<NNN>` | `FALSIFY-SM-001` |
| Kani harness ID | `KANI-<XX>-<NNN>` | `KANI-SM-001` |
| QA gate ID | `F-<XX>-<NNN>` | `F-SM-001` |
| Obligation prefix | `<XX>-<TYPE>-<NNN>` | `SM-INV-001` |

Where `<XX>` is a 2-3 letter kernel abbreviation:

| Kernel | Prefix |
|---|---|
| Softmax | SM |
| RMSNorm | RN |
| RoPE | RP |
| Activation | ACT |
| Attention | ATT |
| MatMul | MM |
| Flash Attention | FA |
| SwiGLU | SG |
| GQA | GQ |
| LayerNorm | LN |
| SiLU | SI |
| Cross-Entropy | CE |
| AdamW | AW |

### 11.3 Version Evolution

Contracts follow semantic versioning:

| Change | Version bump | Example |
|---|---|---|
| Add obligation | Minor (0.x.0) | Add SIMD equivalence test |
| Tighten tolerance | Minor (0.x.0) | 1e-4 → 1e-6 |
| Change equation | Major (x.0.0) | Different softmax formulation |
| Add falsification test | Patch (0.0.x) | New boundary test |
| Fix typo | Patch (0.0.x) | Fix domain description |

---

## References

1. Popper, K. (1963). *Conjectures and Refutations*. Routledge.
2. Ohno, T. (1988). *Toyota Production System*. Productivity Press.
3. Shingo, S. (1986). *Zero Quality Control*. Productivity Press.
4. Vaswani et al. (2017). "Attention Is All You Need." NeurIPS.
5. Su et al. (2021). "RoFormer: Enhanced Transformer with Rotary
   Position Embedding." arXiv:2104.09864.
6. Zhang & Sennrich (2019). "Root Mean Square Layer Normalization."
   NeurIPS.
7. Dao et al. (2022). "FlashAttention: Fast and Memory-Efficient
   Exact Attention with IO-Awareness." NeurIPS.
8. Dao (2023). "FlashAttention-2: Faster Attention with Better
   Parallelism and Work Partitioning." ICLR.
9. Hendrycks & Gimpel (2016). "Gaussian Error Linear Units."
   arXiv:1606.08415.
10. Shazeer (2020). "GLU Variants Improve Transformer."
    arXiv:2002.05202.
11. Ainslie et al. (2023). "GQA: Training Generalized Multi-Query
    Transformer Models from Multi-Head Checkpoints." EMNLP.
12. Goto & van de Geijn (2008). "Anatomy of High-Performance Matrix
    Multiplication." ACM TOMS.
13. Dettmers et al. (2022). "LLM.int8(): 8-bit Matrix Multiplication
    for Transformers at Scale." NeurIPS.
14. Ba et al. (2016). "Layer Normalization." arXiv:1607.06450.
15. Ramachandran et al. (2017). "Searching for Activation Functions."
    arXiv:1710.05941.
16. Nair & Hinton (2010). "Rectified Linear Units Improve Restricted
    Boltzmann Machines." ICML.
17. Bridle (1990). "Training Stochastic Model Recognition Algorithms
    as Networks." NATO ASI Series.
18. Milakov & Gimelshein (2018). "Online normalizer calculation for
    softmax." arXiv:1805.02867.
19. Touvron et al. (2023). "Llama 2: Open Foundation and Fine-Tuned
    Chat Models." arXiv:2307.09288.
20. Kingma & Ba (2014). "Adam: A Method for Stochastic Optimization."
    ICLR 2015.
21. Loshchilov & Hutter (2017). "Decoupled Weight Decay
    Regularization." ICLR 2019.
22. Williams et al. (2009). "Roofline: An Insightful Visual
    Performance Model for Multicore Architectures." CACM.
23. Frantar et al. (2022). "GPTQ: Accurate Post-Training Quantization
    for Generative Pre-trained Transformers." arXiv:2210.17323.
24. Brin & Page (1998). "The Anatomy of a Large-Scale Hypertextual Web
    Search Engine." WWW.
25. Lloyd (1982). "Least Squares Quantization in PCM." IEEE Trans IT.
