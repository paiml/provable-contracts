# Binding Registry: provable-contracts <-> aprender
#
# Maps each kernel contract equation to the aprender function
# that implements it. Used by `pv audit --binding` and
# `pv probar --binding` to generate wired property tests.
#
# Status values:
#   implemented   — function exists and matches contract semantics
#   partial       — function exists but does not cover all obligations
#   not_implemented — no public function available

version: "1.0.0"
target_crate: aprender

bindings:
  - contract: softmax-kernel-v1.yaml
    equation: softmax
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "2D tensors only; dim parameter currently unused"

  - contract: rmsnorm-kernel-v1.yaml
    equation: rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Struct-based; requires RMSNorm::new(shape) then .forward()"

  - contract: rope-kernel-v1.yaml
    equation: rope
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "4D tensor [batch, seq, heads, head_dim]; head_dim must be even"

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: implemented
    notes: "Module-private in aprender; full public impl in realizar::gpu::scheduler::ops::gqa_multihead_attention"

  - contract: activation-kernel-v1.yaml
    equation: gelu
    module_path: "aprender::nn::functional::gelu"
    function: gelu
    signature: "fn gelu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: relu
    module_path: "aprender::nn::functional::relu"
    function: relu
    signature: "fn relu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar, AVX2, and PTX implementations in provable-contracts kernels"

  - contract: matmul-kernel-v1.yaml
    equation: matmul
    module_path: "aprender::autograd::Tensor"
    function: "Tensor::matmul"
    signature: "fn matmul(&self, other: &Tensor) -> Tensor"
    status: implemented
    notes: "2D only; delegates to trueno SIMD"

  - contract: matmul-kernel-v1.yaml
    equation: quantized_dot
    module_path: "realizar::quantize::fused_k"
    function: fused_q4k_dot
    signature: "fn fused_q4k_dot(q4k_data: &[u8], activations: &[f32]) -> Result<f32>"
    status: implemented
    notes: "Scalar, AVX2, AVX-512 VNNI variants; also Q6K, Q8_0 in realizar::gpu"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "realizar::gguf::inference::attention_part_02"
    function: flash_attention_tiled
    signature: "fn flash_attention_tiled(q: &[f32], k: &[f32], v: &[f32], ...) -> Vec<f32>"
    status: implemented
    notes: "CPU tiled + CUDA flash attention with causal masking in realizar"

  # --- New contracts (Tier 1/2) ---

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "aprender::models::qwen2"
    function: swiglu_fused
    signature: "fn swiglu_fused(gate: &Tensor, up: &Tensor) -> Tensor"
    status: implemented
    notes: "Private in aprender qwen2; full public impls in realizar: fused_swiglu_host/gpu + FusedFfnBrick"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "SiLU component shared with silu-kernel-v1; scalar/AVX2/PTX in provable-contracts"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "aprender::nn::transformer"
    function: "MultiHeadAttention::forward"
    signature: "fn forward(&self, query: &Tensor, key: &Tensor, value: &Tensor, mask: Option<&Tensor>) -> Tensor"
    status: implemented
    notes: "Embedded in aprender MHA; standalone pub impl in realizar::gpu::scheduler::ops::gqa_multihead_attention with SIMD"

  - contract: layernorm-kernel-v1.yaml
    equation: layernorm
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard LayerNorm with gamma/beta affine parameters"

  - contract: layernorm-kernel-v1.yaml
    equation: statistics
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Mean and variance computed internally within forward()"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::silu_standalone"
    function: silu_standalone_scalar
    signature: "fn silu_standalone_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar, AVX2, PTX variants in provable-contracts; also in realizar::gguf::ops"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "provable_contracts::kernels::silu_standalone"
    function: sigmoid_scalar
    signature: "fn sigmoid_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar, AVX2, PTX variants; also in trueno backends (all 6 architectures)"

  - contract: cross-entropy-kernel-v1.yaml
    equation: cross_entropy
    module_path: "aprender::nn::CrossEntropyLoss"
    function: "CrossEntropyLoss::forward"
    signature: "fn forward(&self, input: &Tensor, target: &Tensor) -> Tensor"
    status: implemented
    notes: "Uses log-sum-exp internally; supports class weights"

  - contract: cross-entropy-kernel-v1.yaml
    equation: log_softmax
    module_path: "provable_contracts::kernels::cross_entropy"
    function: log_softmax_scalar
    signature: "fn log_softmax_scalar(logits: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar and AVX2 variants in provable-contracts; used internally by CrossEntropyLoss"

  - contract: adamw-kernel-v1.yaml
    equation: adam_moments
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Moment updates computed inside step(); not separately testable"

  - contract: adamw-kernel-v1.yaml
    equation: adam_variance
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Second moment update inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: bias_correction
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Bias correction applied inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: weight_update
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Decoupled weight decay applied after Adam update in step()"

  # --- Tier 3 contracts (not yet implemented in aprender) ---

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: conv1d-kernel-v1.yaml
    equation: conv1d
    module_path: "aprender::nn::Conv1d"
    function: "Conv1d::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard 1D convolution; used in Whisper encoder"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_train
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Training mode with batch statistics"

  - contract: batchnorm-kernel-v1.yaml
    equation: running_stats
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Running stats updated during forward() in training mode"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_eval
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Eval mode uses running stats; controlled by .eval() flag"

  - contract: kmeans-kernel-v1.yaml
    equation: assignment
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::predict"
    signature: "fn predict(&self, x: &Tensor) -> Vec<usize>"
    status: implemented
    notes: "Nearest centroid assignment"

  - contract: kmeans-kernel-v1.yaml
    equation: update
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::fit"
    signature: "fn fit(&mut self, x: &Tensor)"
    status: implemented
    notes: "Centroid update inside fit() loop"

  - contract: kmeans-kernel-v1.yaml
    equation: objective
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::inertia"
    signature: "fn inertia(&self) -> f64"
    status: implemented
    notes: "Sum of squared distances to centroids"

  - contract: pagerank-kernel-v1.yaml
    equation: pagerank
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Power iteration PageRank"

  - contract: pagerank-kernel-v1.yaml
    equation: power_iteration
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Iteration loop inside pagerank()"

  - contract: lbfgs-kernel-v1.yaml
    equation: two_loop_recursion
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Two-loop recursion inside step()"

  - contract: lbfgs-kernel-v1.yaml
    equation: secant_condition
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Secant pairs stored and curvature checked"

  - contract: lbfgs-kernel-v1.yaml
    equation: line_search
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Strong Wolfe line search inside step()"

  - contract: cma-es-kernel-v1.yaml
    equation: sample
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::ask"
    signature: "fn ask(&mut self) -> Vec<Vec<f64>>"
    status: implemented
    notes: "Sample population from N(m, sigma^2*C)"

  - contract: cma-es-kernel-v1.yaml
    equation: mean_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Weighted recombination of best mu solutions"

  - contract: cma-es-kernel-v1.yaml
    equation: covariance_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Rank-one and rank-mu covariance update"

  # --- Model-architecture contracts (Qwen showcase + Qwen3 specs) ---

  - contract: model-config-algebra-v1.yaml
    equation: divisibility
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — config constraints tested via proptest strategies (no API needed)"

  - contract: model-config-algebra-v1.yaml
    equation: bounds
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dimension bounds verified algebraically"

  - contract: model-config-algebra-v1.yaml
    equation: cross_constraint
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Float parameter constraints (rope_theta, eps)"

  - contract: qk-norm-v1.yaml
    equation: qk_rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Per-head QK normalization uses RMSNorm; tested via RMSNorm::without_affine"

  - contract: qk-norm-v1.yaml
    equation: qk_rmsnorm
    module_path: "trueno_gpu::kernels::layernorm::PerHeadRmsNormKernel"
    function: "PerHeadRmsNormKernel::build_ptx"
    signature: "fn build_ptx(&self) -> PtxKernel"
    status: implemented
    notes: >
      GPU per-head RMSNorm using rsqrt.approx and warp shuffle reduction.
      Launched via realizar::per_head_rmsnorm_into(). Cross-backend equivalence
      tested by FALSIFY-QKN-005 (GPU reference simulation vs CPU RMSNorm::forward).

  - contract: tensor-shape-flow-v1.yaml
    equation: qkv_projection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — shape arithmetic tested via proptest (no matmul needed)"

  - contract: tensor-shape-flow-v1.yaml
    equation: gqa_grouping
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "GQA grouping divisibility checked algebraically"

  - contract: tensor-shape-flow-v1.yaml
    equation: swiglu_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "SwiGLU dimension chain verified algebraically"

  - contract: roofline-model-v1.yaml
    equation: bandwidth_ceiling
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — roofline algebra tested inline (no aprender API needed)"

  - contract: roofline-model-v1.yaml
    equation: throughput_bound
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Throughput bounded by min(bw_ceiling, compute_ceiling)"

  - contract: gated-delta-net-v1.yaml
    equation: decay
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_decay"
    signature: "fn gated_delta_net_decay(a_log: &[f32], dt: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Decay in gated_delta_rule_step(): decay = exp(g_h), S *= decay (line 648-652 of linear_attn.rs)"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_write"
    signature: "fn gated_delta_net_write(state: &mut [f32], k: &[f32], delta: &[f32], decay: &[f32])"
    status: implemented
    notes: "State write in gated_delta_rule_step(): S += k ⊗ delta (line 670-680 of linear_attn.rs)"

  - contract: gated-delta-net-v1.yaml
    equation: output
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_output"
    signature: "fn gated_delta_net_output(y: &[f32], z: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Output in gated_delta_rule_step(): o = S^T @ q, then rms_norm_gated (line 682-694 of linear_attn.rs)"

  - contract: format-parity-v1.yaml
    equation: transpose_involution
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — swap(swap(shape)) == shape tested via proptest"

  - contract: format-parity-v1.yaml
    equation: element_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Element count preservation tested algebraically"

  - contract: format-parity-v1.yaml
    equation: identity_1d
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "1D tensor identity mapping tested via proptest"

  # --- Batch 2 contracts (GH-288/290/291/292/293/295/298) ---

  - contract: shannon-entropy-v1.yaml
    equation: entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Shannon entropy tested inline (no aprender API needed)"

  - contract: shannon-entropy-v1.yaml
    equation: uniform_entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Uniform entropy monotonicity tested via proptest"

  - contract: f16-conversion-v1.yaml
    equation: f16_to_f32_bias
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — IEEE 754 bias trick tested inline"

  - contract: f16-conversion-v1.yaml
    equation: roundtrip
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "f16→f32→f16 roundtrip tested for normal bit patterns"

  - contract: kernel-launch-budget-v1.yaml
    equation: per_token_launches
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — kernel count formula tested via proptest"

  - contract: kernel-launch-budget-v1.yaml
    equation: per_layer_decomposition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Static decomposition sum verified (2+5+1+1+1+2=12)"

  - contract: kernel-launch-budget-v1.yaml
    equation: bsum_budget
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Launch count monotonicity tested via proptest"

  - contract: tensor-inventory-v1.yaml
    equation: tensor_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — tensor count formula tested with random configs"

  - contract: tensor-inventory-v1.yaml
    equation: architecture_delta
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Delta between architectures proportional to L"

  - contract: tensor-inventory-v1.yaml
    equation: parameter_decomposition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Tied embeddings reduce count by exactly 1"

  - contract: tensor-inventory-v1.yaml
    equation: quantization_bytes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Quantization size ordering: Q4K < Q6K < Q8 < F16 < F32"

  - contract: performance-grading-v1.yaml
    equation: ollama_parity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — grade boundaries exhaustive and monotonic"

  - contract: performance-grading-v1.yaml
    equation: efficiency_grade
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Efficiency grade monotonic via proptest"

  - contract: performance-grading-v1.yaml
    equation: concrete_instance
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DDR4 33 GB/s, 4.19 GB model => ~7.9 tok/s verified"

  - contract: lora-algebra-v1.yaml
    equation: task_vector
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — task vector roundtrip tested via proptest"

  - contract: lora-algebra-v1.yaml
    equation: eckart_young
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Rank-1 matrix SVD recovery tested (simplified Eckart-Young)"

  - contract: lora-algebra-v1.yaml
    equation: lora_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "A[m,r] @ B[r,n] = [m,n] shape compatibility verified"

  - contract: lora-algebra-v1.yaml
    equation: dare_unbiased
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DARE unbiasedness: (1-p) * 1/(1-p) * delta = delta"

  - contract: lora-algebra-v1.yaml
    equation: shape_preservation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Merge preserves base tensor shapes"

  - contract: quantization-ordering-v1.yaml
    equation: size_ordering
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — strict size ordering tested via proptest"

  - contract: quantization-ordering-v1.yaml
    equation: bytes_per_param
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Concrete Qwen3.5 9B sizes within 20% of expected"

  - contract: quantization-ordering-v1.yaml
    equation: alpha_scaling
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "LoRA alpha/rank scaling verified (16/64=0.25)"

  - contract: quantization-ordering-v1.yaml
    equation: dropout_expectation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DARE dropout rescale expectation = 1.0"

  # --- Batch 3 contracts (GH-286/287/289/294/296/297/299/300) ---

  - contract: q4k-q6k-superblock-v1.yaml
    equation: q4k_superblock
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Q4K 144 bytes = 2+2+12+128 for 256 elements"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: q6k_superblock
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Q6K 210 bytes = 128+64+16+2 for 256 elements"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: total_bytes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Monotonic total bytes for quantized weight matrix"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: dequantization
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dequant produces finite values, offset vanishing tested"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: bsum
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "bsum depends only on quantized values, not scales"

  - contract: sampling-algorithms-v1.yaml
    equation: greedy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — greedy == argmax tested via proptest"

  - contract: sampling-algorithms-v1.yaml
    equation: top_k
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "At most K nonzero probabilities after filtering"

  - contract: sampling-algorithms-v1.yaml
    equation: top_p
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Cumulative probability >= p for retained tokens"

  - contract: sampling-algorithms-v1.yaml
    equation: temperature
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "T=1 identity: softmax(l/1) == softmax(l)"

  - contract: validated-tensor-v1.yaml
    equation: density_gate
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Density > 0.055 for valid embeddings, rejects sparse"

  - contract: validated-tensor-v1.yaml
    equation: nan_inf_rejection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "NaN and Inf injection detected"

  - contract: validated-tensor-v1.yaml
    equation: l2_norm_nondegeneracy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Zero rows detected, non-zero rows pass"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: hybrid_dispatch
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — exhaustive partition of layer types"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: linear_associativity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Scalar multiplication associativity as proxy for matrix associativity"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: linear_shapes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Head grouping exact and residual shape preserved"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: conv1d_causal
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Causal conv1d output length == input length"

  - contract: qwen35-shapes-v1.yaml
    equation: q_projection_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Qwen3.5-9B: 16 * 256 = 4096 (square Q projection)"

  - contract: qwen35-shapes-v1.yaml
    equation: kv_projection_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Qwen3.5-9B: 4 * 256 = 1024, GQA ratio = 4"

  - contract: qwen35-shapes-v1.yaml
    equation: swiglu_ratio
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "12288 / 4096 = 3.0"

  - contract: qwen35-shapes-v1.yaml
    equation: rope_frequency
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "RoPE frequencies strictly decreasing, freq_0 = 1.0"

  - contract: kv-cache-sizing-v1.yaml
    equation: per_token_per_layer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "kv_bytes = 2 * n_kv * d_k * bpe"

  - contract: kv-cache-sizing-v1.yaml
    equation: total_kv_memory
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Monotonic in sequence length and layer count"

  - contract: kv-cache-sizing-v1.yaml
    equation: hybrid_accounting
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Only attention layers contribute to KV cache"

  - contract: kv-cache-sizing-v1.yaml
    equation: bias_absence
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "W @ zeros = zeros when no bias"

  - contract: backend-dispatch-v1.yaml
    equation: gpu_threshold
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dispatch monotonic: larger tensors get higher dispatch level"

  - contract: backend-dispatch-v1.yaml
    equation: garbage_oracle
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Repetitive text detected, diverse text passes"

  - contract: backend-dispatch-v1.yaml
    equation: qk_norm_score_bound
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dot product of unit vectors bounded by sqrt(d)"

  - contract: kv-cache-equivalence-v1.yaml
    equation: prefill_incremental
    module_path: "realizar::gguf::inference::forward::batch_part_02_part_03"
    function: prefill_batch
    signature: "fn prefill_batch(model: &mut Model, tokens: &[u32], ...) -> Result<Vec<f32>>"
    status: implemented
    notes: "Batched and chunked prefill in realizar; KV cache populated incrementally"

  - contract: kv-cache-equivalence-v1.yaml
    equation: page_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "page_elements = block_size * n_kv * d_k"

  - contract: kv-cache-equivalence-v1.yaml
    equation: batched_serial_equivalence
    module_path: "realizar::gguf::inference::forward"
    function: generate_with_batched_prefill
    signature: "fn generate_with_batched_prefill(...) -> Result<Vec<u32>>"
    status: implemented
    notes: "Batched prefill produces same KV cache as serial; tested via FALSIFY-KV gates"

  - contract: kv-cache-equivalence-v1.yaml
    equation: fused_kernel
    module_path: "realizar::cuda::executor::kv_cache_part_02_part_03"
    function: flash_attention_cached
    signature: "fn flash_attention_cached(q: &CudaSlice, k_cache: &CudaSlice, v_cache: &CudaSlice, ...) -> Result<()>"
    status: implemented
    notes: "CUDA flash attention with fused KV cache read in realizar"

  # --- Qwen 3.5 extended contracts ---

  - contract: sliding-window-attention-v1.yaml
    equation: window_mask
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — window mask algebra tested via proptest"

  - contract: sliding-window-attention-v1.yaml
    equation: causal_window_mask
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — causal constraint tested via proptest"

  - contract: sliding-window-attention-v1.yaml
    equation: effective_context
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — min(i+1, W) formula tested"

  - contract: sliding-window-attention-v1.yaml
    equation: attention_sparsity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — sparsity asymptote verified"

  - contract: sliding-window-attention-v1.yaml
    equation: multi_layer_receptive_field
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — receptive field linear growth tested"

  - contract: rope-extrapolation-v1.yaml
    equation: base_frequency
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::new"
    signature: "fn new(dim: usize, max_seq_len: usize, base: f32) -> Self"
    status: implemented
    notes: "Base frequency computed in constructor"

  - contract: rope-extrapolation-v1.yaml
    equation: ntk_scaled_base
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Ntk and DynamicNtk in realizar; tested in position_part_04_part_02"

  - contract: rope-extrapolation-v1.yaml
    equation: linear_interpolation
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Linear in realizar; context_multiplier tested"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_ramp
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Yarn with ramp function in realizar; auto_attn_factor tested"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_mixed_frequency
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "YaRN mixed-frequency applied via ScaledRope::apply(); tested in position_part_04_part_02"

  - contract: rope-extrapolation-v1.yaml
    equation: rotation_matrix
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "Rotation applied per-head; orthogonality is intrinsic to cos/sin construction"

  - contract: embedding-algebra-v1.yaml
    equation: embedding_lookup
    module_path: "aprender::nn::Embedding"
    function: "Embedding::forward"
    signature: "fn forward(&self, input: &[usize]) -> Tensor"
    status: implemented
    notes: "Standard embedding lookup by token ID"

  - contract: embedding-algebra-v1.yaml
    equation: unembedding_projection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Uses matmul with transposed embedding weight (lm_head)"

  - contract: embedding-algebra-v1.yaml
    equation: tied_weights
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Weight tying configured in Qwen2 model builder"

  - contract: embedding-algebra-v1.yaml
    equation: vocabulary_bounds
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — bounds checked at tokenization layer"

  - contract: embedding-algebra-v1.yaml
    equation: embedding_norm
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Non-degeneracy tested via proptest on random embeddings"

  - contract: embedding-algebra-v1.yaml
    equation: logit_temperature
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — temperature scaling tested inline"

  - contract: inference-pipeline-v1.yaml
    equation: prefill_phase
    module_path: "realizar::gguf::inference::forward::batch_part_02_part_03"
    function: prefill_batch
    signature: "fn prefill_batch(model: &mut Model, tokens: &[u32], ...) -> Result<Vec<f32>>"
    status: implemented
    notes: "Batched and chunked prefill in realizar; 8.2x speedup over serial"

  - contract: inference-pipeline-v1.yaml
    equation: decode_step
    module_path: "realizar::gguf::inference::forward"
    function: "Model::forward"
    signature: "fn forward(&mut self, token: u32, position: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "Single-token forward pass in realizar; decode is the default path after prefill"

  - contract: inference-pipeline-v1.yaml
    equation: residual_stream
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — residual arithmetic tested via proptest"

  - contract: inference-pipeline-v1.yaml
    equation: layer_composition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — pre-norm + residual composition tested"

  - contract: inference-pipeline-v1.yaml
    equation: hybrid_layer_schedule
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — partition exhaustiveness tested via proptest"

  - contract: inference-pipeline-v1.yaml
    equation: kv_cache_growth
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — linear growth in t tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: attention_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — shape preservation and residual tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gdn_sublayer
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_forward"
    signature: "fn gated_delta_net_forward(input: &[f32], weights: &GdnWeights, state: &mut GdnState) -> Vec<f32>"
    status: implemented
    notes: "forward_linear_block_with_cache() in linear_attn.rs: pre-norm → projections → causal Conv1D → SiLU → gated_delta_rule_step → RMSNorm_gated → out_proj → residual"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: ffn_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — SwiGLU shape round-trip tested"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: hybrid_block
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — exclusive OR layer type tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: activation_magnitude
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — RMSNorm bounding tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gradient_flow
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — residual identity Jacobian verified"

  - contract: attention-scaling-v1.yaml
    equation: scaled_dot_product
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: implemented
    notes: "1/sqrt(d_k) scaling applied in both aprender SDPA and realizar GQA attention kernels"

  - contract: attention-scaling-v1.yaml
    equation: variance_preservation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — variance ≈ 1 for scaled dot product tested via proptest"

  - contract: attention-scaling-v1.yaml
    equation: softmax_saturation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — entropy reduction with magnitude tested"

  - contract: attention-scaling-v1.yaml
    equation: score_bound_with_qknorm
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Cauchy-Schwarz bound on unit-norm vectors"

  - contract: attention-scaling-v1.yaml
    equation: attention_entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — entropy bounds [0, log(m)] tested via proptest"

  - contract: attention-scaling-v1.yaml
    equation: numerical_stability
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "Max-subtraction trick implemented in softmax"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: model_parameter_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — parameter sum formula tested with Qwen3.5 constants"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: flops_per_token
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — 2P approximation tested via proptest"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: memory_breakdown
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — memory components additive and ordered"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: throughput_model
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — roofline min(bw, compute) tested via proptest"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: verification_ladder
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Meta-contract — coverage fraction tested via pv coverage"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: contract_composition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — compositional shape preservation tested"

  # --- Core kernel bindings (Tier 1/2) ---

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "realizar::apr::helpers"
    function: simple_attention
    signature: "fn simple_attention(q: &[f32], k: &[f32], v: &[f32], seq_len: usize, num_heads: usize, num_kv_heads: usize, head_dim: usize) -> Vec<f32>"
    status: implemented
    notes: "Multi-sequence causal attention in realizar; GQA-aware with head broadcasting"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "realizar::gguf::inference::attention_part_02"
    function: flash_attention_tiled
    signature: "fn flash_attention_tiled(q: &[f32], k: &[f32], v: &[f32], ...) -> Vec<f32>"
    status: implemented
    notes: "CPU tiled + CUDA flash attention (causal, multi-head, cached) in realizar"

  - contract: gated-delta-net-v1.yaml
    equation: decay
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_decay"
    signature: "fn gated_delta_net_decay(a_log: &[f32], dt: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Decay in gated_delta_rule_step(): decay = exp(g_h), S *= decay (line 648-652)"

  - contract: gated-delta-net-v1.yaml
    equation: read
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_read"
    signature: "fn gated_delta_net_read(state: &[f32], q: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "State read in gated_delta_rule_step(): mem = S^T @ k_norm (line 654-665)"

  - contract: gated-delta-net-v1.yaml
    equation: delta
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_delta"
    signature: "fn gated_delta_net_delta(v: &[f32], read: &[f32], beta: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Delta rule in gated_delta_rule_step(): delta = beta * (v - mem) (line 667-680)"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_write"
    signature: "fn gated_delta_net_write(state: &mut [f32], k: &[f32], delta: &[f32], decay: &[f32])"
    status: implemented
    notes: "State write in gated_delta_rule_step(): S += k ⊗ delta (line 670-680)"

  - contract: gated-delta-net-v1.yaml
    equation: output
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_output"
    signature: "fn gated_delta_net_output(y: &[f32], z: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Output in gated_delta_rule_step(): o = S^T @ q_norm (line 682-694)"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "realizar::apr::helpers"
    function: simple_attention
    signature: "fn simple_attention(q: &[f32], k: &[f32], v: &[f32], seq_len: usize, num_heads: usize, num_kv_heads: usize, head_dim: usize) -> Vec<f32>"
    status: implemented
    notes: "GQA with head broadcasting (num_heads/num_kv_heads grouping) in realizar"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::silu_standalone"
    function: silu_standalone_scalar
    signature: "fn silu_standalone_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar/AVX2/PTX in provable-contracts; also in realizar::gguf::ops::silu"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "provable_contracts::kernels::silu_standalone"
    function: sigmoid_scalar
    signature: "fn sigmoid_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar/AVX2/PTX; also in trueno backends (6 architectures)"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented; requires zero-order hold discretization (Mamba paper §3.2)"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented; requires parallel associative scan (Mamba paper §3.3)"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented; requires input-dependent selection (Mamba paper §3.4)"

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "provable_contracts::kernels::swiglu"
    function: swiglu_scalar
    signature: "fn swiglu_scalar(gate: &[f32], value: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar/AVX2/PTX; also fused in realizar::quantize::activation_part_02 and CUDA"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Shared with silu-kernel-v1; SiLU component of SwiGLU"

  # --- Qwen 3.5 binding gap fills ---

  - contract: rope-extrapolation-v1.yaml
    equation: ntk_scaled_base
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Ntk and DynamicNtk in realizar"

  - contract: rope-extrapolation-v1.yaml
    equation: linear_interpolation
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Linear with context_multiplier in realizar"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_ramp
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Yarn with ramp function in realizar"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_mixed_frequency
    module_path: "realizar::layers::position"
    function: "ScaledRope::apply"
    signature: "fn apply(&self, x: &[f32], position: usize, head_dim: usize) -> Vec<f32>"
    status: implemented
    notes: "YaRN mixed-frequency via ScaledRope in realizar"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gdn_sublayer
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_forward"
    signature: "fn gated_delta_net_forward(input: &[f32], weights: &GdnWeights, state: &mut GdnState) -> Vec<f32>"
    status: implemented
    notes: "Full 216-line impl in realizar::gpu::scheduler::linear_attn::forward_linear_block_incremental — Conv1D, SiLU, delta rule, L2 norm"

  - contract: inference-pipeline-v1.yaml
    equation: prefill_phase
    module_path: "realizar::gguf::inference::forward::batch_part_02_part_03"
    function: prefill_batch
    signature: "fn prefill_batch(model: &mut Model, tokens: &[u32], ...) -> Result<Vec<f32>>"
    status: implemented
    notes: "Batched prefill in realizar; 8.2x speedup over serial"

  - contract: inference-pipeline-v1.yaml
    equation: decode_step
    module_path: "realizar::gguf::inference::forward"
    function: "Model::forward"
    signature: "fn forward(&mut self, token: u32, position: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "Single-token decode via KV-cached forward pass in realizar"

  - contract: qwen35-shapes-v1.yaml
    equation: o_projection_transpose
    module_path: "realizar::gguf::config"
    function: "ArchConstraints::from_gguf_metadata"
    signature: "fn from_gguf_metadata(metadata: &GgufMetadata) -> ArchConstraints"
    status: implemented
    notes: "O projection shape [hidden, n_h * d_k] validated by ArchConstraints in realizar"

  # --- GH-279: Architecture Requirements (Weight Loading Validation) ---

  - contract: architecture-requirements-v1.yaml
    equation: gguf_loader_validation
    module_path: "realizar::cuda::executor::weights"
    function: "CudaExecutor::build_indexed_weights"
    signature: "fn build_indexed_weights(&mut self, num_layers: usize, layer_prefix_fn: F, arch: &ArchConstraints) -> Result<(), GpuError>"
    status: implemented
    notes: "GGUF loader path — validates via ValidatedLayerWeights::validate()"

  - contract: architecture-requirements-v1.yaml
    equation: apr_loader_validation
    module_path: "realizar::apr::cuda_part_02_part_03"
    function: "AprV2ModelCuda::upload_quantized_weights_to_gpu"
    signature: "fn upload_quantized_weights_to_gpu(&mut self) -> Result<usize>"
    status: implemented
    notes: "APR loader path — derives ArchConstraints from metadata, validates via build_indexed_weights"

  - contract: architecture-requirements-v1.yaml
    equation: safetensors_loader_validation
    module_path: "realizar::safetensors_cuda"
    function: "SafeTensorsCudaModel::upload_weights"
    signature: "fn upload_weights(...) -> Result<(Vec<f32>, ...)>"
    status: implemented
    notes: "SafeTensors loader calls validate_safetensors_completeness() + validate_model_load_basic() before GPU init (GH-279)"

  - contract: architecture-requirements-v1.yaml
    equation: import_completeness_gate
    module_path: "aprender::format::layout_contract"
    function: "enforce_architecture_completeness"
    signature: "fn enforce_architecture_completeness(tensor_names: &[&str], architecture: &str, num_layers: usize) -> Result<(), ContractError>"
    status: implemented
    notes: "Import/export boundary check — rejects incomplete models before writing APR file"

  # --- Kernel reference implementations (provable-contracts::kernels) ---
  # These map each compute kernel contract to its scalar/AVX2/PTX reference
  # implementation in the provable-contracts crate's kernels module.

  - contract: activation-kernel-v1.yaml
    equation: relu
    module_path: "provable_contracts::kernels::activation"
    function: relu_scalar
    signature: "fn relu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: activation-kernel-v1.yaml
    equation: gelu
    module_path: "provable_contracts::kernels::activation"
    function: gelu_scalar
    signature: "fn gelu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: activation-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::silu_standalone"
    function: silu_standalone_scalar
    signature: "fn silu_standalone_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "provable_contracts::kernels::silu_standalone"
    function: sigmoid_scalar
    signature: "fn sigmoid_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: softmax-kernel-v1.yaml
    equation: softmax
    module_path: "provable_contracts::kernels::softmax"
    function: softmax_scalar
    signature: "fn softmax_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: rmsnorm-kernel-v1.yaml
    equation: rmsnorm
    module_path: "provable_contracts::kernels::rmsnorm"
    function: rmsnorm_scalar
    signature: "fn rmsnorm_scalar(input: &[f32], gamma: &[f32], eps: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: layernorm-kernel-v1.yaml
    equation: layernorm
    module_path: "provable_contracts::kernels::layernorm"
    function: layernorm_scalar
    signature: "fn layernorm_scalar(input: &[f32], gamma: &[f32], beta: &[f32], eps: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_train
    module_path: "provable_contracts::kernels::batchnorm"
    function: batchnorm_scalar
    signature: "fn batchnorm_scalar(input: &[f32], n: usize, c: usize, gamma: &[f32], beta: &[f32], eps: f32, output: &mut [f32], mean: &mut [f32], var: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "provable_contracts::kernels::swiglu"
    function: swiglu_scalar
    signature: "fn swiglu_scalar(gate: &[f32], value: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: cross-entropy-kernel-v1.yaml
    equation: cross_entropy
    module_path: "provable_contracts::kernels::cross_entropy"
    function: cross_entropy_scalar
    signature: "fn cross_entropy_scalar(targets: &[f32], logits: &[f32], num_classes: usize) -> f32"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: rope-kernel-v1.yaml
    equation: rope
    module_path: "provable_contracts::kernels::rope"
    function: rope_scalar
    signature: "fn rope_scalar(x: &[f32], position: usize, dim: usize, base: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: matmul-kernel-v1.yaml
    equation: matmul
    module_path: "provable_contracts::kernels::matmul"
    function: matmul_scalar
    signature: "fn matmul_scalar(a: &[f32], b: &[f32], m: usize, p: usize, n: usize, c: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "provable_contracts::kernels::attention"
    function: attention_scalar
    signature: "fn attention_scalar(q: &[f32], k: &[f32], v: &[f32], n: usize, m: usize, d_k: usize, d_v: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "provable_contracts::kernels::gqa"
    function: gqa_scalar
    signature: "fn gqa_scalar(q: &[f32], k: &[f32], v: &[f32], n: usize, d_k: usize, d_v: usize, num_heads: usize, num_kv_heads: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "provable_contracts::kernels::flash_attention"
    function: flash_attention_scalar
    signature: "fn flash_attention_scalar(q: &[f32], k: &[f32], v: &[f32], n: usize, d: usize, tile_size: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: adamw-kernel-v1.yaml
    equation: weight_update
    module_path: "provable_contracts::kernels::adamw"
    function: adamw_step_scalar
    signature: "fn adamw_step_scalar(params: &mut [f32], grads: &[f32], m: &mut [f32], v: &mut [f32], lr: f32, beta1: f32, beta2: f32, eps: f32, wd: f32, t: u32)"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: conv1d-kernel-v1.yaml
    equation: conv1d
    module_path: "provable_contracts::kernels::conv1d"
    function: conv1d_scalar
    signature: "fn conv1d_scalar(input: &[f32], weight: &[f32], bias: Option<&[f32]>, c_in: usize, c_out: usize, l: usize, k: usize, stride: usize, pad: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: "provable_contracts::kernels::ssm"
    function: ssm_scan_scalar
    signature: "fn ssm_scan_scalar(a_bar: &[f32], b_bar: &[f32], c: &[f32], x: &[f32], n: usize, l: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: kmeans-kernel-v1.yaml
    equation: assignment
    module_path: "provable_contracts::kernels::kmeans"
    function: kmeans_assign_scalar
    signature: "fn kmeans_assign_scalar(points: &[f32], centroids: &[f32], n: usize, k: usize, d: usize, assignments: &mut [u32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: pagerank-kernel-v1.yaml
    equation: pagerank
    module_path: "provable_contracts::kernels::pagerank"
    function: pagerank_iterate_scalar
    signature: "fn pagerank_iterate_scalar(transition: &[f32], rank: &[f32], n: usize, damping: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: lbfgs-kernel-v1.yaml
    equation: two_loop_recursion
    module_path: "provable_contracts::kernels::lbfgs"
    function: lbfgs_direction_scalar
    signature: "fn lbfgs_direction_scalar(gradient: &[f32], s_history: &[Vec<f32>], y_history: &[Vec<f32>], direction: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: cma-es-kernel-v1.yaml
    equation: sample
    module_path: "provable_contracts::kernels::cma_es"
    function: cma_sample_scalar
    signature: "fn cma_sample_scalar(mean: &[f32], sigma: f32, cholesky_l: &[f32], d: usize, z: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "provable_contracts::kernels::gated_delta_net"
    function: gdn_recurrence_scalar
    signature: "fn gdn_recurrence_scalar(state: &mut [f32], k: &[f32], v: &[f32], q: &[f32], z: &[f32], alpha: f32, beta: f32, k_dim: usize, v_dim: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  # ============================================================
  # Algorithm Contracts (Phase 6 — UCBD v2.0.0 §14-15)
  # ============================================================

  # --- metrics-regression-v1 ---

  - contract: metrics-regression-v1.yaml
    equation: r_squared
    module_path: "aprender::metrics"
    function: r_squared
    signature: "fn r_squared(y_pred: &Vector<f32>, y_true: &Vector<f32>) -> f32"
    status: implemented
    notes: "R² = 1 - SS_res/SS_tot"

  - contract: metrics-regression-v1.yaml
    equation: mse
    module_path: "aprender::metrics"
    function: mse
    signature: "fn mse(y_pred: &Vector<f32>, y_true: &Vector<f32>) -> f32"
    status: implemented
    notes: "Mean Squared Error"

  - contract: metrics-regression-v1.yaml
    equation: mae
    module_path: "aprender::metrics"
    function: mae
    signature: "fn mae(y_pred: &Vector<f32>, y_true: &Vector<f32>) -> f32"
    status: implemented
    notes: "Mean Absolute Error"

  - contract: metrics-regression-v1.yaml
    equation: rmse
    module_path: "aprender::metrics"
    function: rmse
    signature: "fn rmse(y_pred: &Vector<f32>, y_true: &Vector<f32>) -> f32"
    status: implemented
    notes: "Root Mean Squared Error = sqrt(MSE)"

  # --- metrics-classification-v1 ---

  - contract: metrics-classification-v1.yaml
    equation: accuracy
    module_path: "aprender::metrics::classification"
    function: accuracy
    signature: "fn accuracy(y_pred: &[usize], y_true: &[usize]) -> f32"
    status: implemented
    notes: "Classification accuracy = correct/total"

  - contract: metrics-classification-v1.yaml
    equation: precision
    module_path: "aprender::metrics::classification"
    function: precision
    signature: "fn precision(y_pred: &[usize], y_true: &[usize], average: Average) -> f32"
    status: implemented
    notes: "TP / (TP + FP), supports Macro/Micro/Weighted"

  - contract: metrics-classification-v1.yaml
    equation: recall
    module_path: "aprender::metrics::classification"
    function: recall
    signature: "fn recall(y_pred: &[usize], y_true: &[usize], average: Average) -> f32"
    status: implemented
    notes: "TP / (TP + FN), supports Macro/Micro/Weighted"

  - contract: metrics-classification-v1.yaml
    equation: f1_score
    module_path: "aprender::metrics::classification"
    function: f1_score
    signature: "fn f1_score(y_pred: &[usize], y_true: &[usize], average: Average) -> f32"
    status: implemented
    notes: "Harmonic mean of precision and recall"

  - contract: metrics-classification-v1.yaml
    equation: confusion_matrix
    module_path: "aprender::metrics::classification"
    function: confusion_matrix
    signature: "fn confusion_matrix(y_pred: &[usize], y_true: &[usize]) -> Matrix<f32>"
    status: implemented
    notes: "CM[i,j] = count of true=i, pred=j"

  # --- metrics-clustering-v1 ---

  - contract: metrics-clustering-v1.yaml
    equation: silhouette_score
    module_path: "aprender::metrics"
    function: silhouette_score
    signature: "fn silhouette_score(data: &Matrix<f32>, labels: &[usize]) -> f32"
    status: implemented
    notes: "Mean silhouette coefficient across all samples"

  - contract: metrics-clustering-v1.yaml
    equation: inertia
    module_path: "aprender::metrics"
    function: inertia
    signature: "fn inertia(data: &Matrix<f32>, centroids: &Matrix<f32>, labels: &[usize]) -> f32"
    status: implemented
    notes: "Within-cluster sum of squares"

  - contract: metrics-clustering-v1.yaml
    equation: silhouette_coefficient
    module_path: "aprender::metrics"
    function: silhouette_coefficient
    signature: "fn silhouette_coefficient(a_i: f32, b_i: f32) -> f32"
    status: implemented
    notes: "Private helper — per-point silhouette"

  # --- loss-functions-v1 ---

  - contract: loss-functions-v1.yaml
    equation: mse_loss
    module_path: "aprender::nn::loss::MSELoss"
    function: "MSELoss::forward"
    signature: "fn forward(&self, predictions: &Tensor, targets: &Tensor) -> Tensor"
    status: implemented
    notes: "MSE loss with autograd support"

  - contract: loss-functions-v1.yaml
    equation: l1_loss
    module_path: "aprender::nn::loss::L1Loss"
    function: "L1Loss::forward"
    signature: "fn forward(&self, predictions: &Tensor, targets: &Tensor) -> Tensor"
    status: implemented
    notes: "L1/MAE loss with autograd support"

  - contract: loss-functions-v1.yaml
    equation: smooth_l1
    module_path: "aprender::nn::loss::SmoothL1Loss"
    function: "SmoothL1Loss::forward"
    signature: "fn forward(&self, predictions: &Tensor, targets: &Tensor) -> Tensor"
    status: implemented
    notes: "Smooth L1 loss (Huber-like)"

  - contract: loss-functions-v1.yaml
    equation: bce
    module_path: "aprender::nn::loss::BCELoss"
    function: "BCELoss::forward"
    signature: "fn forward(&self, predictions: &Tensor, targets: &Tensor) -> Tensor"
    status: implemented
    notes: "Binary Cross-Entropy loss"

  - contract: loss-functions-v1.yaml
    equation: nll
    module_path: "aprender::nn::loss::NLLLoss"
    function: "NLLLoss::forward"
    signature: "fn forward(&self, predictions: &Tensor, targets: &Tensor) -> Tensor"
    status: implemented
    notes: "Negative Log-Likelihood loss"

  - contract: loss-functions-v1.yaml
    equation: huber
    module_path: "aprender::nn::loss::HuberLoss"
    function: "HuberLoss::forward"
    signature: "fn forward(&self, predictions: &Tensor, targets: &Tensor) -> Tensor"
    status: implemented
    notes: "Huber loss with configurable delta"

  # --- graph-centrality-v1 ---

  - contract: graph-centrality-v1.yaml
    equation: degree
    module_path: "aprender::graph::centrality"
    function: "Graph::degree_centrality"
    signature: "fn degree_centrality(&self) -> HashMap<NodeId, f64>"
    status: implemented
    notes: "Freeman's normalization: deg(v)/(n-1)"

  - contract: graph-centrality-v1.yaml
    equation: betweenness
    module_path: "aprender::graph::centrality"
    function: "Graph::betweenness_centrality"
    signature: "fn betweenness_centrality(&self) -> HashMap<NodeId, f64>"
    status: implemented
    notes: "Brandes algorithm"

  - contract: graph-centrality-v1.yaml
    equation: closeness
    module_path: "aprender::graph::centrality"
    function: "Graph::closeness_centrality"
    signature: "fn closeness_centrality(&self) -> HashMap<NodeId, f64>"
    status: implemented
    notes: "Geodesic distance based"

  - contract: graph-centrality-v1.yaml
    equation: eigenvector
    module_path: "aprender::graph::centrality"
    function: "Graph::eigenvector_centrality"
    signature: "fn eigenvector_centrality(&self, max_iter: usize, tol: f64) -> Result<HashMap<NodeId, f64>, &'static str>"
    status: implemented
    notes: "Power iteration for dominant eigenvector"

  - contract: graph-centrality-v1.yaml
    equation: katz
    module_path: "aprender::graph::centrality"
    function: "Graph::katz_centrality"
    signature: "fn katz_centrality(&self, alpha: f64, beta: f64, max_iter: usize, tol: f64) -> Result<HashMap<NodeId, f64>, &'static str>"
    status: implemented
    notes: "Katz centrality with attenuation factor"

  - contract: graph-centrality-v1.yaml
    equation: harmonic
    module_path: "aprender::graph::centrality"
    function: "Graph::harmonic_centrality"
    signature: "fn harmonic_centrality(&self) -> HashMap<NodeId, f64>"
    status: implemented
    notes: "Boldi & Vigna (2014), handles disconnected graphs"

  # ────────────────────────────────────────────────────────
  # P1 Algorithm Contracts: Calibration
  # ────────────────────────────────────────────────────────

  - contract: calibration-v1.yaml
    equation: expected_calibration_error
    module_path: "aprender::calibration"
    function: "expected_calibration_error"
    signature: "fn expected_calibration_error(predictions: &[f32], labels: &[bool], n_bins: usize) -> f32"
    status: implemented
    notes: "ECE with uniform-width binning"

  - contract: calibration-v1.yaml
    equation: maximum_calibration_error
    module_path: "aprender::calibration"
    function: "maximum_calibration_error"
    signature: "fn maximum_calibration_error(predictions: &[f32], labels: &[bool], n_bins: usize) -> f32"
    status: implemented
    notes: "MCE — worst-case bin calibration error"

  - contract: calibration-v1.yaml
    equation: platt_scaling
    module_path: "aprender::calibration"
    function: "PlattScaling::fit"
    signature: "fn fit(logits: &[f32], labels: &[bool]) -> Self"
    status: implemented
    notes: "Sigmoid calibration via log-loss minimization"

  - contract: calibration-v1.yaml
    equation: isotonic_regression
    module_path: "aprender::calibration"
    function: "IsotonicRegression::fit"
    signature: "fn fit(&mut self, predictions: &[f32], labels: &[bool]) -> Result<()>"
    status: implemented
    notes: "Pool Adjacent Violators (PAV) algorithm"

  - contract: calibration-v1.yaml
    equation: reliability_diagram
    module_path: "aprender::calibration"
    function: "reliability_diagram"
    signature: "fn reliability_diagram(predictions: &[f32], labels: &[bool], n_bins: usize) -> Vec<(f32, f32, usize)>"
    status: implemented
    notes: "Bin-level (confidence, accuracy, count) tuples"

  # ────────────────────────────────────────────────────────
  # P1 Algorithm Contracts: Preprocessing Normalization
  # ────────────────────────────────────────────────────────

  - contract: preprocessing-normalization-v1.yaml
    equation: standard_scaler
    module_path: "aprender::preprocessing"
    function: "StandardScaler::fit + StandardScaler::transform"
    signature: "fn fit(&mut self, x: &Matrix<f32>) -> Result<()>, fn transform(&self, x: &Matrix<f32>) -> Result<Matrix<f32>>"
    status: implemented
    notes: "Zero-mean unit-variance standardization"

  - contract: preprocessing-normalization-v1.yaml
    equation: minmax_scaler
    module_path: "aprender::preprocessing"
    function: "MinMaxScaler::fit + MinMaxScaler::transform"
    signature: "fn fit(&mut self, x: &Matrix<f32>) -> Result<()>, fn transform(&self, x: &Matrix<f32>) -> Result<Matrix<f32>>"
    status: implemented
    notes: "Feature scaling to [min, max] range"

  - contract: preprocessing-normalization-v1.yaml
    equation: robust_scaler
    module_path: "aprender::preprocessing"
    function: "RobustScaler::fit + RobustScaler::transform"
    signature: "fn fit(&mut self, x: &Matrix<f32>) -> Result<()>, fn transform(&self, x: &Matrix<f32>) -> Result<Matrix<f32>>"
    status: not_implemented
    notes: "Robust scaling with median/IQR — not yet implemented"

  # ────────────────────────────────────────────────────────
  # P1 Algorithm Contracts: Decision Tree
  # ────────────────────────────────────────────────────────

  - contract: decision-tree-v1.yaml
    equation: gini_impurity
    module_path: "aprender::tree::helpers"
    function: "gini_impurity"
    signature: "fn gini_impurity(labels: &[usize]) -> f32"
    status: implemented
    notes: "Gini impurity for CART splitting criterion"

  - contract: decision-tree-v1.yaml
    equation: gini_split
    module_path: "aprender::tree::helpers"
    function: "gini_split"
    signature: "fn gini_split(left_labels: &[usize], right_labels: &[usize]) -> f32"
    status: implemented
    notes: "Weighted Gini impurity for binary split"

  - contract: decision-tree-v1.yaml
    equation: mse_split
    module_path: "aprender::tree::helpers"
    function: "mse_impurity"
    signature: "fn mse_impurity(targets: &[f32]) -> f32"
    status: not_implemented
    notes: "MSE impurity for regression trees"

  - contract: decision-tree-v1.yaml
    equation: prediction
    module_path: "aprender::tree::classifier"
    function: "DecisionTreeClassifier::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vec<usize>>"
    status: implemented
    notes: "CART classifier prediction via tree traversal"

  # ────────────────────────────────────────────────────────
  # P1 Algorithm Contracts: Naive Bayes
  # ────────────────────────────────────────────────────────

  - contract: naive-bayes-v1.yaml
    equation: class_prior
    module_path: "aprender::classification"
    function: "GaussianNB::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Computes class priors, means, variances during fit"

  - contract: naive-bayes-v1.yaml
    equation: gaussian_likelihood
    module_path: "aprender::classification"
    function: "GaussianNB::predict_proba"
    signature: "fn predict_proba(&self, x: &Matrix<f32>) -> Result<Vec<Vec<f32>>>"
    status: implemented
    notes: "Gaussian PDF evaluation per class per feature"

  - contract: naive-bayes-v1.yaml
    equation: log_posterior
    module_path: "aprender::classification"
    function: "GaussianNB::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vec<usize>>"
    status: implemented
    notes: "Argmax of log-posteriors for classification"

  # ────────────────────────────────────────────────────────
  # P2 Algorithm Contracts: Linear Models
  # ────────────────────────────────────────────────────────

  - contract: linear-models-v1.yaml
    equation: ols_fit
    module_path: "aprender::linear_model"
    function: "LinearRegression::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &Vector<f32>) -> Result<()>"
    status: implemented
    notes: "OLS via normal equations (Cholesky solver)"

  - contract: linear-models-v1.yaml
    equation: ols_predict
    module_path: "aprender::linear_model"
    function: "LinearRegression::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vector<f32>>"
    status: implemented
    notes: "Linear prediction: ŷ = Xβ + b"

  - contract: linear-models-v1.yaml
    equation: r_squared_training
    module_path: "aprender::linear_model"
    function: "LinearRegression::score"
    signature: "fn score(&self, x: &Matrix<f32>, y: &Vector<f32>) -> f32"
    status: implemented
    notes: "R² = 1 - SS_res/SS_tot on training data"

  - contract: linear-models-v1.yaml
    equation: logistic_predict_proba
    module_path: "aprender::classification"
    function: "LogisticRegression::predict_proba"
    signature: "fn predict_proba(&self, x: &Matrix<f32>) -> Result<Vec<f32>>"
    status: implemented
    notes: "Sigmoid probability output"

  # ────────────────────────────────────────────────────────
  # P2 Algorithm Contracts: SVM
  # ────────────────────────────────────────────────────────

  - contract: svm-v1.yaml
    equation: hinge_loss
    module_path: "aprender::classification"
    function: "LinearSVM::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Subgradient descent with hinge loss"

  - contract: svm-v1.yaml
    equation: decision_function
    module_path: "aprender::classification"
    function: "LinearSVM::decision_function"
    signature: "fn decision_function(&self, x: &Matrix<f32>) -> Result<Vec<f32>>"
    status: implemented
    notes: "w·x + b decision values"

  - contract: svm-v1.yaml
    equation: svm_predict
    module_path: "aprender::classification"
    function: "LinearSVM::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vec<usize>>"
    status: implemented
    notes: "Binary classification via sign(decision_function)"

  # ────────────────────────────────────────────────────────
  # P2 Algorithm Contracts: PCA
  # ────────────────────────────────────────────────────────

  - contract: pca-v1.yaml
    equation: pca_transform
    module_path: "aprender::preprocessing"
    function: "PCA::fit + PCA::transform"
    signature: "fn fit(&mut self, x: &Matrix<f32>) -> Result<()>, fn transform(&self, x: &Matrix<f32>) -> Result<Matrix<f32>>"
    status: implemented
    notes: "Eigendecomposition-based dimensionality reduction"

  - contract: pca-v1.yaml
    equation: explained_variance
    module_path: "aprender::preprocessing"
    function: "PCA::explained_variance_ratio"
    signature: "fn explained_variance_ratio(&self) -> Option<&[f32]>"
    status: implemented
    notes: "Ratio of variance captured per component"

  # ────────────────────────────────────────────────────────
  # P2 Algorithm Contracts: ICA
  # ────────────────────────────────────────────────────────

  - contract: ica-v1.yaml
    equation: fastica
    module_path: "aprender::decomposition"
    function: "ICA::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>) -> Result<()>"
    status: implemented
    notes: "FastICA with deflation and tanh nonlinearity"

  - contract: ica-v1.yaml
    equation: unmixing
    module_path: "aprender::decomposition"
    function: "ICA::transform"
    signature: "fn transform(&self, x: &Matrix<f32>) -> Result<Matrix<f32>>"
    status: implemented
    notes: "S = X W^T unmixing transformation"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: ARIMA
  # ────────────────────────────────────────────────────────

  - contract: arima-v1.yaml
    equation: ar_forecast
    module_path: "aprender::time_series"
    function: "ARIMA::forecast"
    signature: "fn forecast(&self, n_periods: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "AR(p) forecasting with fitted coefficients"

  - contract: arima-v1.yaml
    equation: differencing
    module_path: "aprender::time_series"
    function: "ARIMA::fit"
    signature: "fn fit(&mut self, data: &[f32]) -> Result<()>"
    status: implemented
    notes: "d-th order differencing applied during fit"

  - contract: arima-v1.yaml
    equation: ma_filter
    module_path: "aprender::time_series"
    function: "ARIMA::forecast"
    signature: "fn forecast(&self, n_periods: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "MA(q) error correction applied during forecast"

  - contract: arima-v1.yaml
    equation: forecast_finite
    module_path: "aprender::time_series"
    function: "ARIMA::forecast"
    signature: "fn forecast(&self, n_periods: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "All forecast values must be finite (no NaN/Inf)"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: Bayesian
  # ────────────────────────────────────────────────────────

  - contract: bayesian-v1.yaml
    equation: conjugate_update
    module_path: "aprender::bayesian::conjugate"
    function: "BetaBinomial::update"
    signature: "fn update(&mut self, successes: usize, trials: usize)"
    status: implemented
    notes: "Beta-Binomial conjugate update: α += successes, β += failures"

  - contract: bayesian-v1.yaml
    equation: posterior_valid
    module_path: "aprender::bayesian::conjugate"
    function: "BetaBinomial::alpha + BetaBinomial::beta"
    signature: "fn alpha(&self) -> f64, fn beta(&self) -> f64"
    status: implemented
    notes: "Posterior parameters must remain positive"

  - contract: bayesian-v1.yaml
    equation: blr_predict
    module_path: "aprender::bayesian::regression"
    function: "BayesianLinearRegression::predict"
    signature: "fn predict(&self, x_test: &Matrix<f32>) -> Result<(Vector<f32>, Vector<f32>)>"
    status: implemented
    notes: "Posterior mean prediction with uncertainty"

  - contract: bayesian-v1.yaml
    equation: posterior_predictive
    module_path: "aprender::bayesian::regression"
    function: "BayesianLinearRegression::predict"
    signature: "fn predict(&self, x_test: &Matrix<f32>) -> Result<(Vector<f32>, Vector<f32>)>"
    status: implemented
    notes: "Returns (mean, std) for predictive distribution"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: GLM
  # ────────────────────────────────────────────────────────

  - contract: glm-v1.yaml
    equation: poisson_link
    module_path: "aprender::glm"
    function: "GLM::new with LinkFunction::Log"
    signature: "fn new(family: Family, link: LinkFunction) -> Self"
    status: implemented
    notes: "Log link for Poisson: g(μ) = ln(μ)"

  - contract: glm-v1.yaml
    equation: gamma_link
    module_path: "aprender::glm"
    function: "GLM::new with LinkFunction::Reciprocal"
    signature: "fn new(family: Family, link: LinkFunction) -> Self"
    status: implemented
    notes: "Reciprocal link for Gamma: g(μ) = 1/μ"

  - contract: glm-v1.yaml
    equation: binomial_link
    module_path: "aprender::glm"
    function: "GLM::new with LinkFunction::Logit"
    signature: "fn new(family: Family, link: LinkFunction) -> Self"
    status: implemented
    notes: "Logit link for Binomial: g(μ) = ln(μ/(1-μ))"

  - contract: glm-v1.yaml
    equation: irls_fit
    module_path: "aprender::glm"
    function: "GLM::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[f32]) -> Result<()>"
    status: implemented
    notes: "IRLS iterative fitting with convergence check"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: Drift Detection
  # ────────────────────────────────────────────────────────

  - contract: drift-detection-v1.yaml
    equation: univariate_drift
    module_path: "aprender::metrics::drift"
    function: "DriftDetector::detect_univariate"
    signature: "fn detect_univariate(&self, reference: &Vector<f32>, current: &Vector<f32>) -> DriftStatus"
    status: implemented
    notes: "Normalized mean difference: |μ_ref - μ_cur| / σ_ref"

  - contract: drift-detection-v1.yaml
    equation: performance_drift
    module_path: "aprender::metrics::drift"
    function: "DriftDetector::detect_performance_drift"
    signature: "fn detect_performance_drift(&self, baseline_scores: &[f32], current_scores: &[f32]) -> DriftStatus"
    status: implemented
    notes: "Score degradation detection"

  - contract: drift-detection-v1.yaml
    equation: classify_drift
    module_path: "aprender::metrics::drift"
    function: "DriftDetector::classify_drift"
    signature: "fn classify_drift(&self, score: f32) -> DriftStatus"
    status: implemented
    notes: "Threshold-based classification into NoDrift/Warning/Drift"

  - contract: drift-detection-v1.yaml
    equation: min_samples_guard
    module_path: "aprender::metrics::drift"
    function: "DriftDetector::detect_univariate"
    signature: "fn detect_univariate(&self, reference: &Vector<f32>, current: &Vector<f32>) -> DriftStatus"
    status: implemented
    notes: "Returns NoDrift when samples < min_samples (avoid false positives)"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: Ranking Metrics
  # ────────────────────────────────────────────────────────

  - contract: metrics-ranking-v1.yaml
    equation: hit_at_k
    module_path: "aprender::metrics::ranking"
    function: "hit_at_k"
    signature: "fn hit_at_k<T: PartialEq>(predictions: &[T], target: &T, k: usize) -> f32"
    status: implemented
    notes: "Binary 0/1 indicator: target in top-K"

  - contract: metrics-ranking-v1.yaml
    equation: reciprocal_rank
    module_path: "aprender::metrics::ranking"
    function: "reciprocal_rank"
    signature: "fn reciprocal_rank<T: PartialEq>(predictions: &[T], target: &T) -> f32"
    status: implemented
    notes: "1/rank of first correct prediction"

  - contract: metrics-ranking-v1.yaml
    equation: mrr
    module_path: "aprender::metrics::ranking"
    function: "mrr"
    signature: "fn mrr<T: PartialEq + Clone>(predictions: &[Vec<T>], targets: &[T]) -> f32"
    status: implemented
    notes: "Mean Reciprocal Rank across queries"

  - contract: metrics-ranking-v1.yaml
    equation: ndcg_at_k
    module_path: "aprender::metrics::ranking"
    function: "ndcg_at_k"
    signature: "fn ndcg_at_k(relevance: &[f32], k: usize) -> f32"
    status: implemented
    notes: "Normalized DCG: DCG/IDCG ∈ [0,1]"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: GBM
  # ────────────────────────────────────────────────────────

  - contract: gbm-v1.yaml
    equation: gradient_boost
    module_path: "aprender::tree::gradient_boosting"
    function: "GradientBoostingClassifier::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Boosting loop: F_m = F_{m-1} + ν·h_m"

  - contract: gbm-v1.yaml
    equation: negative_gradient
    module_path: "aprender::tree::gradient_boosting"
    function: "GradientBoostingClassifier::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Pseudo-residuals computed inside fit loop"

  - contract: gbm-v1.yaml
    equation: predict
    module_path: "aprender::tree::gradient_boosting"
    function: "GradientBoostingClassifier::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vec<usize>>"
    status: implemented
    notes: "Aggregated tree predictions with sigmoid thresholding"

  - contract: gbm-v1.yaml
    equation: training_loss
    module_path: "aprender::tree::gradient_boosting"
    function: "GradientBoostingClassifier::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Loss computed per stage inside fit"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: Optimization
  # ────────────────────────────────────────────────────────

  - contract: optimization-v1.yaml
    equation: cg_minimize
    module_path: "aprender::optim::conjugate_gradient"
    function: "ConjugateGradient::minimize"
    signature: "fn minimize<F, G>(&self, f: F, grad: G, x0: &[f64]) -> Result<OptimizationResult>"
    status: implemented
    notes: "Fletcher-Reeves nonlinear CG with line search"

  - contract: optimization-v1.yaml
    equation: line_search
    module_path: "aprender::optim::conjugate_gradient"
    function: "ConjugateGradient::minimize"
    signature: "fn minimize<F, G>(&self, f: F, grad: G, x0: &[f64]) -> Result<OptimizationResult>"
    status: implemented
    notes: "Backtracking line search inside minimize"

  - contract: optimization-v1.yaml
    equation: convergence
    module_path: "aprender::optim::conjugate_gradient"
    function: "ConjugateGradient::minimize"
    signature: "fn minimize<F, G>(&self, f: F, grad: G, x0: &[f64]) -> Result<OptimizationResult>"
    status: implemented
    notes: "Convergence via gradient norm tolerance"

  # ────────────────────────────────────────────────────────
  # P3 Algorithm Contracts: Random Forest
  # ────────────────────────────────────────────────────────

  - contract: random-forest-v1.yaml
    equation: bootstrap_sample
    module_path: "aprender::tree::random_forest"
    function: "RandomForestClassifier::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Bootstrap sampling with replacement inside fit"

  - contract: random-forest-v1.yaml
    equation: majority_vote
    module_path: "aprender::tree::random_forest"
    function: "RandomForestClassifier::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vec<usize>>"
    status: implemented
    notes: "Majority vote across ensemble trees"

  - contract: random-forest-v1.yaml
    equation: predict
    module_path: "aprender::tree::random_forest"
    function: "RandomForestClassifier::predict"
    signature: "fn predict(&self, x: &Matrix<f32>) -> Result<Vec<usize>>"
    status: implemented
    notes: "Aggregated prediction from bootstrap ensemble"

  - contract: random-forest-v1.yaml
    equation: ensemble_size
    module_path: "aprender::tree::random_forest"
    function: "RandomForestClassifier::fit"
    signature: "fn fit(&mut self, x: &Matrix<f32>, y: &[usize]) -> Result<()>"
    status: implemented
    notes: "Number of trees matches n_estimators configuration"
