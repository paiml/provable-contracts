# Binding Registry: provable-contracts <-> aprender
#
# Maps each kernel contract equation to the aprender function
# that implements it. Used by `pv audit --binding` and
# `pv probar --binding` to generate wired property tests.
#
# Status values:
#   implemented   — function exists and matches contract semantics
#   partial       — function exists but does not cover all obligations
#   not_implemented — no public function available

version: "1.0.0"
target_crate: aprender

bindings:
  - contract: softmax-kernel-v1.yaml
    equation: softmax
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "2D tensors only; dim parameter currently unused"

  - contract: rmsnorm-kernel-v1.yaml
    equation: rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Struct-based; requires RMSNorm::new(shape) then .forward()"

  - contract: rope-kernel-v1.yaml
    equation: rope
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "4D tensor [batch, seq, heads, head_dim]; head_dim must be even"

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: partial
    notes: "Function is module-private (not pub); accessible only through MultiHeadAttention"

  - contract: activation-kernel-v1.yaml
    equation: gelu
    module_path: "aprender::nn::functional::gelu"
    function: gelu
    signature: "fn gelu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: relu
    module_path: "aprender::nn::functional::relu"
    function: relu
    signature: "fn relu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: silu
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Only exists as private fn in models::qwen2; needs public nn::functional::silu"

  - contract: matmul-kernel-v1.yaml
    equation: matmul
    module_path: "aprender::autograd::Tensor"
    function: "Tensor::matmul"
    signature: "fn matmul(&self, other: &Tensor) -> Tensor"
    status: implemented
    notes: "2D only; delegates to trueno SIMD"

  - contract: matmul-kernel-v1.yaml
    equation: quantized_dot
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Quantized dot product not yet implemented in aprender"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Flash attention not yet implemented"

  # --- New contracts (Tier 1/2) ---

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "aprender::models::qwen2"
    function: swiglu_fused
    signature: "fn swiglu_fused(gate: &Tensor, up: &Tensor) -> Tensor"
    status: partial
    notes: "Private fn in models::qwen2; fuses SiLU(gate) * up but not the linear projections"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SiLU standalone is private in qwen2; see also silu-kernel-v1"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "aprender::nn::transformer"
    function: "MultiHeadAttention::forward"
    signature: "fn forward(&self, query: &Tensor, key: &Tensor, value: &Tensor, mask: Option<&Tensor>) -> Tensor"
    status: partial
    notes: "GQA logic embedded in MultiHeadAttention when num_kv_heads < num_heads; not separately testable"

  - contract: layernorm-kernel-v1.yaml
    equation: layernorm
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard LayerNorm with gamma/beta affine parameters"

  - contract: layernorm-kernel-v1.yaml
    equation: statistics
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Mean and variance computed internally within forward()"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SiLU only exists as private fn in models::qwen2; needs public nn::functional::silu"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Sigmoid not exposed as public function"

  - contract: cross-entropy-kernel-v1.yaml
    equation: cross_entropy
    module_path: "aprender::nn::CrossEntropyLoss"
    function: "CrossEntropyLoss::forward"
    signature: "fn forward(&self, input: &Tensor, target: &Tensor) -> Tensor"
    status: implemented
    notes: "Uses log-sum-exp internally; supports class weights"

  - contract: cross-entropy-kernel-v1.yaml
    equation: log_softmax
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "LogSoftmax not exposed as standalone function; internal to CrossEntropyLoss"

  - contract: adamw-kernel-v1.yaml
    equation: adam_moments
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Moment updates computed inside step(); not separately testable"

  - contract: adamw-kernel-v1.yaml
    equation: adam_variance
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Second moment update inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: bias_correction
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Bias correction applied inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: weight_update
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Decoupled weight decay applied after Adam update in step()"

  # --- Tier 3 contracts (not yet implemented in aprender) ---

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: conv1d-kernel-v1.yaml
    equation: conv1d
    module_path: "aprender::nn::Conv1d"
    function: "Conv1d::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard 1D convolution; used in Whisper encoder"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_train
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Training mode with batch statistics"

  - contract: batchnorm-kernel-v1.yaml
    equation: running_stats
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Running stats updated during forward() in training mode"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_eval
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Eval mode uses running stats; controlled by .eval() flag"

  - contract: kmeans-kernel-v1.yaml
    equation: assignment
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::predict"
    signature: "fn predict(&self, x: &Tensor) -> Vec<usize>"
    status: implemented
    notes: "Nearest centroid assignment"

  - contract: kmeans-kernel-v1.yaml
    equation: update
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::fit"
    signature: "fn fit(&mut self, x: &Tensor)"
    status: implemented
    notes: "Centroid update inside fit() loop"

  - contract: kmeans-kernel-v1.yaml
    equation: objective
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::inertia"
    signature: "fn inertia(&self) -> f64"
    status: implemented
    notes: "Sum of squared distances to centroids"

  - contract: pagerank-kernel-v1.yaml
    equation: pagerank
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Power iteration PageRank"

  - contract: pagerank-kernel-v1.yaml
    equation: power_iteration
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Iteration loop inside pagerank()"

  - contract: lbfgs-kernel-v1.yaml
    equation: two_loop_recursion
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Two-loop recursion inside step()"

  - contract: lbfgs-kernel-v1.yaml
    equation: secant_condition
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Secant pairs stored and curvature checked"

  - contract: lbfgs-kernel-v1.yaml
    equation: line_search
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Strong Wolfe line search inside step()"

  - contract: cma-es-kernel-v1.yaml
    equation: sample
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::ask"
    signature: "fn ask(&mut self) -> Vec<Vec<f64>>"
    status: implemented
    notes: "Sample population from N(m, sigma^2*C)"

  - contract: cma-es-kernel-v1.yaml
    equation: mean_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Weighted recombination of best mu solutions"

  - contract: cma-es-kernel-v1.yaml
    equation: covariance_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Rank-one and rank-mu covariance update"

  # --- Model-architecture contracts (Qwen showcase + Qwen3 specs) ---

  - contract: model-config-algebra-v1.yaml
    equation: divisibility
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — config constraints tested via proptest strategies (no API needed)"

  - contract: model-config-algebra-v1.yaml
    equation: bounds
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dimension bounds verified algebraically"

  - contract: model-config-algebra-v1.yaml
    equation: cross_constraint
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Float parameter constraints (rope_theta, eps)"

  - contract: qk-norm-v1.yaml
    equation: qk_rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Per-head QK normalization uses RMSNorm; tested via RMSNorm::without_affine"

  - contract: qk-norm-v1.yaml
    equation: qk_rmsnorm
    module_path: "trueno_gpu::kernels::layernorm::PerHeadRmsNormKernel"
    function: "PerHeadRmsNormKernel::build_ptx"
    signature: "fn build_ptx(&self) -> PtxKernel"
    status: implemented
    notes: >
      GPU per-head RMSNorm using rsqrt.approx and warp shuffle reduction.
      Launched via realizar::per_head_rmsnorm_into(). Cross-backend equivalence
      tested by FALSIFY-QKN-005 (GPU reference simulation vs CPU RMSNorm::forward).

  - contract: tensor-shape-flow-v1.yaml
    equation: qkv_projection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — shape arithmetic tested via proptest (no matmul needed)"

  - contract: tensor-shape-flow-v1.yaml
    equation: gqa_grouping
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "GQA grouping divisibility checked algebraically"

  - contract: tensor-shape-flow-v1.yaml
    equation: swiglu_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "SwiGLU dimension chain verified algebraically"

  - contract: roofline-model-v1.yaml
    equation: bandwidth_ceiling
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — roofline algebra tested inline (no aprender API needed)"

  - contract: roofline-model-v1.yaml
    equation: throughput_bound
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Throughput bounded by min(bw_ceiling, compute_ceiling)"

  - contract: gated-delta-net-v1.yaml
    equation: decay
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Gated Delta Net not yet implemented in aprender (Qwen3.5)"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "State update recurrence not implemented"

  - contract: gated-delta-net-v1.yaml
    equation: output
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Output gating not implemented"

  - contract: format-parity-v1.yaml
    equation: transpose_involution
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — swap(swap(shape)) == shape tested via proptest"

  - contract: format-parity-v1.yaml
    equation: element_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Element count preservation tested algebraically"

  - contract: format-parity-v1.yaml
    equation: identity_1d
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "1D tensor identity mapping tested via proptest"

  # --- Batch 2 contracts (GH-288/290/291/292/293/295/298) ---

  - contract: shannon-entropy-v1.yaml
    equation: entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Shannon entropy tested inline (no aprender API needed)"

  - contract: shannon-entropy-v1.yaml
    equation: uniform_entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Uniform entropy monotonicity tested via proptest"

  - contract: f16-conversion-v1.yaml
    equation: f16_to_f32_bias
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — IEEE 754 bias trick tested inline"

  - contract: f16-conversion-v1.yaml
    equation: roundtrip
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "f16→f32→f16 roundtrip tested for normal bit patterns"

  - contract: kernel-launch-budget-v1.yaml
    equation: per_token_launches
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — kernel count formula tested via proptest"

  - contract: kernel-launch-budget-v1.yaml
    equation: per_layer_decomposition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Static decomposition sum verified (2+5+1+1+1+2=12)"

  - contract: kernel-launch-budget-v1.yaml
    equation: bsum_budget
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Launch count monotonicity tested via proptest"

  - contract: tensor-inventory-v1.yaml
    equation: tensor_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — tensor count formula tested with random configs"

  - contract: tensor-inventory-v1.yaml
    equation: architecture_delta
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Delta between architectures proportional to L"

  - contract: tensor-inventory-v1.yaml
    equation: parameter_decomposition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Tied embeddings reduce count by exactly 1"

  - contract: tensor-inventory-v1.yaml
    equation: quantization_bytes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Quantization size ordering: Q4K < Q6K < Q8 < F16 < F32"

  - contract: performance-grading-v1.yaml
    equation: ollama_parity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — grade boundaries exhaustive and monotonic"

  - contract: performance-grading-v1.yaml
    equation: efficiency_grade
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Efficiency grade monotonic via proptest"

  - contract: performance-grading-v1.yaml
    equation: concrete_instance
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DDR4 33 GB/s, 4.19 GB model => ~7.9 tok/s verified"

  - contract: lora-algebra-v1.yaml
    equation: task_vector
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — task vector roundtrip tested via proptest"

  - contract: lora-algebra-v1.yaml
    equation: eckart_young
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Rank-1 matrix SVD recovery tested (simplified Eckart-Young)"

  - contract: lora-algebra-v1.yaml
    equation: lora_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "A[m,r] @ B[r,n] = [m,n] shape compatibility verified"

  - contract: lora-algebra-v1.yaml
    equation: dare_unbiased
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DARE unbiasedness: (1-p) * 1/(1-p) * delta = delta"

  - contract: lora-algebra-v1.yaml
    equation: shape_preservation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Merge preserves base tensor shapes"

  - contract: quantization-ordering-v1.yaml
    equation: size_ordering
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — strict size ordering tested via proptest"

  - contract: quantization-ordering-v1.yaml
    equation: bytes_per_param
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Concrete Qwen3.5 9B sizes within 20% of expected"

  - contract: quantization-ordering-v1.yaml
    equation: alpha_scaling
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "LoRA alpha/rank scaling verified (16/64=0.25)"

  - contract: quantization-ordering-v1.yaml
    equation: dropout_expectation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DARE dropout rescale expectation = 1.0"

  # --- Batch 3 contracts (GH-286/287/289/294/296/297/299/300) ---

  - contract: q4k-q6k-superblock-v1.yaml
    equation: q4k_superblock
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Q4K 144 bytes = 2+2+12+128 for 256 elements"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: q6k_superblock
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Q6K 210 bytes = 128+64+16+2 for 256 elements"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: total_bytes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Monotonic total bytes for quantized weight matrix"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: dequantization
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dequant produces finite values, offset vanishing tested"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: bsum
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "bsum depends only on quantized values, not scales"

  - contract: sampling-algorithms-v1.yaml
    equation: greedy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — greedy == argmax tested via proptest"

  - contract: sampling-algorithms-v1.yaml
    equation: top_k
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "At most K nonzero probabilities after filtering"

  - contract: sampling-algorithms-v1.yaml
    equation: top_p
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Cumulative probability >= p for retained tokens"

  - contract: sampling-algorithms-v1.yaml
    equation: temperature
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "T=1 identity: softmax(l/1) == softmax(l)"

  - contract: validated-tensor-v1.yaml
    equation: density_gate
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Density > 0.055 for valid embeddings, rejects sparse"

  - contract: validated-tensor-v1.yaml
    equation: nan_inf_rejection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "NaN and Inf injection detected"

  - contract: validated-tensor-v1.yaml
    equation: l2_norm_nondegeneracy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Zero rows detected, non-zero rows pass"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: hybrid_dispatch
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — exhaustive partition of layer types"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: linear_associativity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Scalar multiplication associativity as proxy for matrix associativity"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: linear_shapes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Head grouping exact and residual shape preserved"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: conv1d_causal
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Causal conv1d output length == input length"

  - contract: qwen35-shapes-v1.yaml
    equation: q_projection_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Qwen3.5-9B: 16 * 256 = 4096 (square Q projection)"

  - contract: qwen35-shapes-v1.yaml
    equation: kv_projection_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Qwen3.5-9B: 4 * 256 = 1024, GQA ratio = 4"

  - contract: qwen35-shapes-v1.yaml
    equation: swiglu_ratio
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "12288 / 4096 = 3.0"

  - contract: qwen35-shapes-v1.yaml
    equation: rope_frequency
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "RoPE frequencies strictly decreasing, freq_0 = 1.0"

  - contract: kv-cache-sizing-v1.yaml
    equation: per_token_per_layer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "kv_bytes = 2 * n_kv * d_k * bpe"

  - contract: kv-cache-sizing-v1.yaml
    equation: total_kv_memory
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Monotonic in sequence length and layer count"

  - contract: kv-cache-sizing-v1.yaml
    equation: hybrid_accounting
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Only attention layers contribute to KV cache"

  - contract: kv-cache-sizing-v1.yaml
    equation: bias_absence
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "W @ zeros = zeros when no bias"

  - contract: backend-dispatch-v1.yaml
    equation: gpu_threshold
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dispatch monotonic: larger tensors get higher dispatch level"

  - contract: backend-dispatch-v1.yaml
    equation: garbage_oracle
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Repetitive text detected, diverse text passes"

  - contract: backend-dispatch-v1.yaml
    equation: qk_norm_score_bound
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dot product of unit vectors bounded by sqrt(d)"

  - contract: kv-cache-equivalence-v1.yaml
    equation: prefill_incremental
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Requires realizar inference API — ignored"

  - contract: kv-cache-equivalence-v1.yaml
    equation: page_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "page_elements = block_size * n_kv * d_k"

  - contract: kv-cache-equivalence-v1.yaml
    equation: batched_serial_equivalence
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Requires realizar inference API — ignored"

  - contract: kv-cache-equivalence-v1.yaml
    equation: fused_kernel
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Requires trueno fused kernel API — ignored"

  # --- Qwen 3.5 extended contracts ---

  - contract: sliding-window-attention-v1.yaml
    equation: window_mask
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — window mask algebra tested via proptest"

  - contract: sliding-window-attention-v1.yaml
    equation: causal_window_mask
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — causal constraint tested via proptest"

  - contract: sliding-window-attention-v1.yaml
    equation: effective_context
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — min(i+1, W) formula tested"

  - contract: sliding-window-attention-v1.yaml
    equation: attention_sparsity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — sparsity asymptote verified"

  - contract: sliding-window-attention-v1.yaml
    equation: multi_layer_receptive_field
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — receptive field linear growth tested"

  - contract: rope-extrapolation-v1.yaml
    equation: base_frequency
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::new"
    signature: "fn new(dim: usize, max_seq_len: usize, base: f32) -> Self"
    status: implemented
    notes: "Base frequency computed in constructor"

  - contract: rope-extrapolation-v1.yaml
    equation: ntk_scaled_base
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "NTK-aware scaling not yet implemented in aprender RoPE"

  - contract: rope-extrapolation-v1.yaml
    equation: linear_interpolation
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Position interpolation not yet implemented"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_ramp
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "YaRN interpolation not yet implemented"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_mixed_frequency
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "YaRN mixed-frequency scaling not yet implemented"

  - contract: rope-extrapolation-v1.yaml
    equation: rotation_matrix
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "Rotation applied per-head; orthogonality is intrinsic to cos/sin construction"

  - contract: embedding-algebra-v1.yaml
    equation: embedding_lookup
    module_path: "aprender::nn::Embedding"
    function: "Embedding::forward"
    signature: "fn forward(&self, input: &[usize]) -> Tensor"
    status: implemented
    notes: "Standard embedding lookup by token ID"

  - contract: embedding-algebra-v1.yaml
    equation: unembedding_projection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Uses matmul with transposed embedding weight (lm_head)"

  - contract: embedding-algebra-v1.yaml
    equation: tied_weights
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Weight tying configured in Qwen2 model builder"

  - contract: embedding-algebra-v1.yaml
    equation: vocabulary_bounds
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — bounds checked at tokenization layer"

  - contract: embedding-algebra-v1.yaml
    equation: embedding_norm
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Non-degeneracy tested via proptest on random embeddings"

  - contract: embedding-algebra-v1.yaml
    equation: logit_temperature
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — temperature scaling tested inline"

  - contract: inference-pipeline-v1.yaml
    equation: prefill_phase
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Requires realizar inference API"

  - contract: inference-pipeline-v1.yaml
    equation: decode_step
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Requires realizar inference API"

  - contract: inference-pipeline-v1.yaml
    equation: residual_stream
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — residual arithmetic tested via proptest"

  - contract: inference-pipeline-v1.yaml
    equation: layer_composition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — pre-norm + residual composition tested"

  - contract: inference-pipeline-v1.yaml
    equation: hybrid_layer_schedule
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — partition exhaustiveness tested via proptest"

  - contract: inference-pipeline-v1.yaml
    equation: kv_cache_growth
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — linear growth in t tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: attention_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — shape preservation and residual tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gdn_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "GDN layer not yet integrated in aprender forward pass"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: ffn_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — SwiGLU shape round-trip tested"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: hybrid_block
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — exclusive OR layer type tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: activation_magnitude
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — RMSNorm bounding tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gradient_flow
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — residual identity Jacobian verified"

  - contract: attention-scaling-v1.yaml
    equation: scaled_dot_product
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: partial
    notes: "Function is module-private; 1/sqrt(d_k) scaling applied internally"

  - contract: attention-scaling-v1.yaml
    equation: variance_preservation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — variance ≈ 1 for scaled dot product tested via proptest"

  - contract: attention-scaling-v1.yaml
    equation: softmax_saturation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — entropy reduction with magnitude tested"

  - contract: attention-scaling-v1.yaml
    equation: score_bound_with_qknorm
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Cauchy-Schwarz bound on unit-norm vectors"

  - contract: attention-scaling-v1.yaml
    equation: attention_entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — entropy bounds [0, log(m)] tested via proptest"

  - contract: attention-scaling-v1.yaml
    equation: numerical_stability
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "Max-subtraction trick implemented in softmax"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: model_parameter_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — parameter sum formula tested with Qwen3.5 constants"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: flops_per_token
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — 2P approximation tested via proptest"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: memory_breakdown
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — memory components additive and ordered"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: throughput_model
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — roofline min(bw, compute) tested via proptest"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: verification_ladder
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Meta-contract — coverage fraction tested via pv coverage"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: contract_composition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — compositional shape preservation tested"

  # --- Core kernel bindings (Tier 1/2) ---

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "aprender::nn::functional::attention"
    function: attention
    signature: "fn attention(q: &Tensor, k: &Tensor, v: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — scaled dot-product attention"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "aprender::nn::functional::flash_attention"
    function: flash_attention
    signature: "fn flash_attention(q: &Tensor, k: &Tensor, v: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — IO-aware tiled attention"

  - contract: gated-delta-net-v1.yaml
    equation: decay
    module_path: "aprender::nn::GatedDeltaNet"
    function: "GatedDeltaNet::decay"
    signature: "fn decay(&self, input: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — exponential decay gate"

  - contract: gated-delta-net-v1.yaml
    equation: read
    module_path: "aprender::nn::GatedDeltaNet"
    function: "GatedDeltaNet::read"
    signature: "fn read(&self, state: &Tensor, q: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — state read via query"

  - contract: gated-delta-net-v1.yaml
    equation: delta
    module_path: "aprender::nn::GatedDeltaNet"
    function: "GatedDeltaNet::delta"
    signature: "fn delta(&self, v: &Tensor, read: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — delta rule update"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "aprender::nn::GatedDeltaNet"
    function: "GatedDeltaNet::write"
    signature: "fn write(&self, state: &Tensor, k: &Tensor, delta: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — state write via key-delta outer product"

  - contract: gated-delta-net-v1.yaml
    equation: output
    module_path: "aprender::nn::GatedDeltaNet"
    function: "GatedDeltaNet::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — composed GDN forward pass"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "aprender::nn::functional::gqa"
    function: gqa
    signature: "fn gqa(q: &Tensor, k: &Tensor, v: &Tensor, n_groups: usize) -> Tensor"
    status: not_implemented
    notes: "Planned — grouped-query attention with head broadcasting"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "aprender::nn::functional::silu"
    function: silu
    signature: "fn silu(x: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — SiLU/Swish activation"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "aprender::nn::functional::sigmoid"
    function: sigmoid
    signature: "fn sigmoid(x: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — sigmoid activation (used by SiLU)"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: "aprender::nn::SSM"
    function: "SSM::discretize"
    signature: "fn discretize(&self, delta: &Tensor) -> (Tensor, Tensor)"
    status: not_implemented
    notes: "Planned — zero-order hold discretization"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: "aprender::nn::SSM"
    function: "SSM::scan"
    signature: "fn scan(&self, x: &Tensor, delta: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — parallel associative scan"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: "aprender::nn::SSM"
    function: "SSM::selective_gate"
    signature: "fn selective_gate(&self, x: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — input-dependent selection mechanism"

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "aprender::nn::functional::swiglu"
    function: swiglu
    signature: "fn swiglu(x: &Tensor, gate: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Planned — SwiGLU gated MLP"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: "aprender::nn::functional::silu"
    function: silu
    signature: "fn silu(x: &Tensor) -> Tensor"
    status: not_implemented
    notes: "Shared with silu-kernel-v1; SiLU component of SwiGLU"

  # --- Qwen 3.5 binding gap fills ---

  - contract: rope-extrapolation-v1.yaml
    equation: ntk_scaled_base
    module_path: "aprender::nn::RoPE"
    function: "RoPE::ntk_scaled_base"
    signature: "fn ntk_scaled_base(base: f64, dim: usize, scale: f64) -> f64"
    status: not_implemented
    notes: "NTK-aware base frequency scaling for context extension"

  - contract: rope-extrapolation-v1.yaml
    equation: linear_interpolation
    module_path: "aprender::nn::RoPE"
    function: "RoPE::linear_interpolation"
    signature: "fn linear_interpolation(freqs: &Tensor, scale: f64) -> Tensor"
    status: not_implemented
    notes: "Linear position interpolation for context extension"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_ramp
    module_path: "aprender::nn::RoPE"
    function: "RoPE::yarn_ramp"
    signature: "fn yarn_ramp(dim: usize, low: f64, high: f64) -> Tensor"
    status: not_implemented
    notes: "YaRN ramp function for frequency mixing"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_mixed_frequency
    module_path: "aprender::nn::RoPE"
    function: "RoPE::yarn_mixed"
    signature: "fn yarn_mixed(freqs: &Tensor, ramp: &Tensor) -> Tensor"
    status: not_implemented
    notes: "YaRN mixed frequency interpolation"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gdn_sublayer
    module_path: "aprender::model::qwen35::GDNBlock"
    function: "GDNBlock::forward"
    signature: "fn forward(&self, x: &Tensor) -> Tensor"
    status: not_implemented
    notes: "GDN sublayer: x + gdn(conv1d(rmsnorm(x)))"

  - contract: inference-pipeline-v1.yaml
    equation: prefill_phase
    module_path: "aprender::model::qwen35::Pipeline"
    function: "Pipeline::prefill"
    signature: "fn prefill(&mut self, tokens: &[u32]) -> Tensor"
    status: not_implemented
    notes: "Parallel prefill: embed → L×block → norm → logits"

  - contract: inference-pipeline-v1.yaml
    equation: decode_step
    module_path: "aprender::model::qwen35::Pipeline"
    function: "Pipeline::decode_step"
    signature: "fn decode_step(&mut self, token: u32) -> Tensor"
    status: not_implemented
    notes: "Autoregressive decode: single token through cached pipeline"

  - contract: qwen35-shapes-v1.yaml
    equation: o_projection_transpose
    module_path: "aprender::model::qwen35::Config"
    function: "Config::o_projection_shape"
    signature: "fn o_projection_shape(&self) -> [usize; 2]"
    status: not_implemented
    notes: "Output projection: [hidden, n_h * d_k] = [4096, 4096]"
