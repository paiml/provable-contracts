# Binding Registry: provable-contracts <-> aprender
#
# Maps each kernel contract equation to the aprender function
# that implements it. Used by `pv audit --binding` and
# `pv probar --binding` to generate wired property tests.
#
# Status values:
#   implemented   — function exists and matches contract semantics
#   partial       — function exists but does not cover all obligations
#   not_implemented — no public function available

version: "1.0.0"
target_crate: aprender

bindings:
  - contract: softmax-kernel-v1.yaml
    equation: softmax
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "2D tensors only; dim parameter currently unused"

  - contract: rmsnorm-kernel-v1.yaml
    equation: rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Struct-based; requires RMSNorm::new(shape) then .forward()"

  - contract: rope-kernel-v1.yaml
    equation: rope
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "4D tensor [batch, seq, heads, head_dim]; head_dim must be even"

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: partial
    notes: "Function is module-private (not pub); accessible only through MultiHeadAttention"

  - contract: activation-kernel-v1.yaml
    equation: gelu
    module_path: "aprender::nn::functional::gelu"
    function: gelu
    signature: "fn gelu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: relu
    module_path: "aprender::nn::functional::relu"
    function: relu
    signature: "fn relu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: silu
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Only exists as private fn in models::qwen2; needs public nn::functional::silu"

  - contract: matmul-kernel-v1.yaml
    equation: matmul
    module_path: "aprender::autograd::Tensor"
    function: "Tensor::matmul"
    signature: "fn matmul(&self, other: &Tensor) -> Tensor"
    status: implemented
    notes: "2D only; delegates to trueno SIMD"

  - contract: matmul-kernel-v1.yaml
    equation: quantized_dot
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Quantized dot product not yet implemented in aprender"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Flash attention not yet implemented"

  # --- New contracts (Tier 1/2) ---

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "aprender::models::qwen2"
    function: swiglu_fused
    signature: "fn swiglu_fused(gate: &Tensor, up: &Tensor) -> Tensor"
    status: partial
    notes: "Private fn in models::qwen2; fuses SiLU(gate) * up but not the linear projections"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SiLU standalone is private in qwen2; see also silu-kernel-v1"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "aprender::nn::transformer"
    function: "MultiHeadAttention::forward"
    signature: "fn forward(&self, query: &Tensor, key: &Tensor, value: &Tensor, mask: Option<&Tensor>) -> Tensor"
    status: partial
    notes: "GQA logic embedded in MultiHeadAttention when num_kv_heads < num_heads; not separately testable"

  - contract: layernorm-kernel-v1.yaml
    equation: layernorm
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard LayerNorm with gamma/beta affine parameters"

  - contract: layernorm-kernel-v1.yaml
    equation: statistics
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Mean and variance computed internally within forward()"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SiLU only exists as private fn in models::qwen2; needs public nn::functional::silu"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "Sigmoid not exposed as public function"

  - contract: cross-entropy-kernel-v1.yaml
    equation: cross_entropy
    module_path: "aprender::nn::CrossEntropyLoss"
    function: "CrossEntropyLoss::forward"
    signature: "fn forward(&self, input: &Tensor, target: &Tensor) -> Tensor"
    status: implemented
    notes: "Uses log-sum-exp internally; supports class weights"

  - contract: cross-entropy-kernel-v1.yaml
    equation: log_softmax
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "LogSoftmax not exposed as standalone function; internal to CrossEntropyLoss"

  - contract: adamw-kernel-v1.yaml
    equation: adam_moments
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Moment updates computed inside step(); not separately testable"

  - contract: adamw-kernel-v1.yaml
    equation: adam_variance
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Second moment update inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: bias_correction
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Bias correction applied inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: weight_update
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Decoupled weight decay applied after Adam update in step()"

  # --- Tier 3 contracts (not yet implemented in aprender) ---

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: conv1d-kernel-v1.yaml
    equation: conv1d
    module_path: "aprender::nn::Conv1d"
    function: "Conv1d::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard 1D convolution; used in Whisper encoder"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_train
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Training mode with batch statistics"

  - contract: batchnorm-kernel-v1.yaml
    equation: running_stats
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Running stats updated during forward() in training mode"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_eval
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Eval mode uses running stats; controlled by .eval() flag"

  - contract: kmeans-kernel-v1.yaml
    equation: assignment
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::predict"
    signature: "fn predict(&self, x: &Tensor) -> Vec<usize>"
    status: implemented
    notes: "Nearest centroid assignment"

  - contract: kmeans-kernel-v1.yaml
    equation: update
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::fit"
    signature: "fn fit(&mut self, x: &Tensor)"
    status: implemented
    notes: "Centroid update inside fit() loop"

  - contract: kmeans-kernel-v1.yaml
    equation: objective
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::inertia"
    signature: "fn inertia(&self) -> f64"
    status: implemented
    notes: "Sum of squared distances to centroids"

  - contract: pagerank-kernel-v1.yaml
    equation: pagerank
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Power iteration PageRank"

  - contract: pagerank-kernel-v1.yaml
    equation: power_iteration
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Iteration loop inside pagerank()"

  - contract: lbfgs-kernel-v1.yaml
    equation: two_loop_recursion
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Two-loop recursion inside step()"

  - contract: lbfgs-kernel-v1.yaml
    equation: secant_condition
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Secant pairs stored and curvature checked"

  - contract: lbfgs-kernel-v1.yaml
    equation: line_search
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Strong Wolfe line search inside step()"

  - contract: cma-es-kernel-v1.yaml
    equation: sample
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::ask"
    signature: "fn ask(&mut self) -> Vec<Vec<f64>>"
    status: implemented
    notes: "Sample population from N(m, sigma^2*C)"

  - contract: cma-es-kernel-v1.yaml
    equation: mean_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Weighted recombination of best mu solutions"

  - contract: cma-es-kernel-v1.yaml
    equation: covariance_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Rank-one and rank-mu covariance update"
