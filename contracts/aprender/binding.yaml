# Binding Registry: provable-contracts <-> aprender
#
# Maps each kernel contract equation to the aprender function
# that implements it. Used by `pv audit --binding` and
# `pv probar --binding` to generate wired property tests.
#
# Status values:
#   implemented   — function exists and matches contract semantics
#   partial       — function exists but does not cover all obligations
#   not_implemented — no public function available

version: "1.0.0"
target_crate: aprender

bindings:
  - contract: softmax-kernel-v1.yaml
    equation: softmax
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "2D tensors only; dim parameter currently unused"

  - contract: rmsnorm-kernel-v1.yaml
    equation: rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Struct-based; requires RMSNorm::new(shape) then .forward()"

  - contract: rope-kernel-v1.yaml
    equation: rope
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "4D tensor [batch, seq, heads, head_dim]; head_dim must be even"

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: implemented
    notes: "Module-private in aprender; full public impl in realizar::gpu::scheduler::ops::gqa_multihead_attention"

  - contract: activation-kernel-v1.yaml
    equation: gelu
    module_path: "aprender::nn::functional::gelu"
    function: gelu
    signature: "fn gelu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: relu
    module_path: "aprender::nn::functional::relu"
    function: relu
    signature: "fn relu(x: &Tensor) -> Tensor"
    status: implemented

  - contract: activation-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar, AVX2, and PTX implementations in provable-contracts kernels"

  - contract: matmul-kernel-v1.yaml
    equation: matmul
    module_path: "aprender::autograd::Tensor"
    function: "Tensor::matmul"
    signature: "fn matmul(&self, other: &Tensor) -> Tensor"
    status: implemented
    notes: "2D only; delegates to trueno SIMD"

  - contract: matmul-kernel-v1.yaml
    equation: quantized_dot
    module_path: "realizar::quantize::fused_k"
    function: fused_q4k_dot
    signature: "fn fused_q4k_dot(q4k_data: &[u8], activations: &[f32]) -> Result<f32>"
    status: implemented
    notes: "Scalar, AVX2, AVX-512 VNNI variants; also Q6K, Q8_0 in realizar::gpu"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "realizar::gguf::inference::attention_part_02"
    function: flash_attention_tiled
    signature: "fn flash_attention_tiled(q: &[f32], k: &[f32], v: &[f32], ...) -> Vec<f32>"
    status: implemented
    notes: "CPU tiled + CUDA flash attention with causal masking in realizar"

  # --- New contracts (Tier 1/2) ---

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "aprender::models::qwen2"
    function: swiglu_fused
    signature: "fn swiglu_fused(gate: &Tensor, up: &Tensor) -> Tensor"
    status: implemented
    notes: "Private in aprender qwen2; full public impls in realizar: fused_swiglu_host/gpu + FusedFfnBrick"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "SiLU component shared with silu-kernel-v1; scalar/AVX2/PTX in provable-contracts"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "aprender::nn::transformer"
    function: "MultiHeadAttention::forward"
    signature: "fn forward(&self, query: &Tensor, key: &Tensor, value: &Tensor, mask: Option<&Tensor>) -> Tensor"
    status: implemented
    notes: "Embedded in aprender MHA; standalone pub impl in realizar::gpu::scheduler::ops::gqa_multihead_attention with SIMD"

  - contract: layernorm-kernel-v1.yaml
    equation: layernorm
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard LayerNorm with gamma/beta affine parameters"

  - contract: layernorm-kernel-v1.yaml
    equation: statistics
    module_path: "aprender::nn::LayerNorm"
    function: "LayerNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Mean and variance computed internally within forward()"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::silu_standalone"
    function: silu_standalone_scalar
    signature: "fn silu_standalone_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar, AVX2, PTX variants in provable-contracts; also in realizar::gguf::ops"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "provable_contracts::kernels::silu_standalone"
    function: sigmoid_scalar
    signature: "fn sigmoid_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar, AVX2, PTX variants; also in trueno backends (all 6 architectures)"

  - contract: cross-entropy-kernel-v1.yaml
    equation: cross_entropy
    module_path: "aprender::nn::CrossEntropyLoss"
    function: "CrossEntropyLoss::forward"
    signature: "fn forward(&self, input: &Tensor, target: &Tensor) -> Tensor"
    status: implemented
    notes: "Uses log-sum-exp internally; supports class weights"

  - contract: cross-entropy-kernel-v1.yaml
    equation: log_softmax
    module_path: "provable_contracts::kernels::cross_entropy"
    function: log_softmax_scalar
    signature: "fn log_softmax_scalar(logits: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar and AVX2 variants in provable-contracts; used internally by CrossEntropyLoss"

  - contract: adamw-kernel-v1.yaml
    equation: adam_moments
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Moment updates computed inside step(); not separately testable"

  - contract: adamw-kernel-v1.yaml
    equation: adam_variance
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Second moment update inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: bias_correction
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Bias correction applied inside step()"

  - contract: adamw-kernel-v1.yaml
    equation: weight_update
    module_path: "aprender::nn::optim::AdamW"
    function: "AdamW::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grads: &[Tensor])"
    status: implemented
    notes: "Decoupled weight decay applied after Adam update in step()"

  # --- Tier 3 contracts (not yet implemented in aprender) ---

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented in aprender"

  - contract: conv1d-kernel-v1.yaml
    equation: conv1d
    module_path: "aprender::nn::Conv1d"
    function: "Conv1d::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Standard 1D convolution; used in Whisper encoder"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_train
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Training mode with batch statistics"

  - contract: batchnorm-kernel-v1.yaml
    equation: running_stats
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Running stats updated during forward() in training mode"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_eval
    module_path: "aprender::nn::BatchNorm1d"
    function: "BatchNorm1d::forward"
    signature: "fn forward(&mut self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Eval mode uses running stats; controlled by .eval() flag"

  - contract: kmeans-kernel-v1.yaml
    equation: assignment
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::predict"
    signature: "fn predict(&self, x: &Tensor) -> Vec<usize>"
    status: implemented
    notes: "Nearest centroid assignment"

  - contract: kmeans-kernel-v1.yaml
    equation: update
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::fit"
    signature: "fn fit(&mut self, x: &Tensor)"
    status: implemented
    notes: "Centroid update inside fit() loop"

  - contract: kmeans-kernel-v1.yaml
    equation: objective
    module_path: "aprender::cluster::KMeans"
    function: "KMeans::inertia"
    signature: "fn inertia(&self) -> f64"
    status: implemented
    notes: "Sum of squared distances to centroids"

  - contract: pagerank-kernel-v1.yaml
    equation: pagerank
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Power iteration PageRank"

  - contract: pagerank-kernel-v1.yaml
    equation: power_iteration
    module_path: "aprender::graph::pagerank"
    function: pagerank
    signature: "fn pagerank(adj: &Tensor, damping: f64, max_iter: usize, tol: f64) -> Tensor"
    status: implemented
    notes: "Iteration loop inside pagerank()"

  - contract: lbfgs-kernel-v1.yaml
    equation: two_loop_recursion
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Two-loop recursion inside step()"

  - contract: lbfgs-kernel-v1.yaml
    equation: secant_condition
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Secant pairs stored and curvature checked"

  - contract: lbfgs-kernel-v1.yaml
    equation: line_search
    module_path: "aprender::optim::LBFGS"
    function: "LBFGS::step"
    signature: "fn step(&mut self, params: &mut [Tensor], grad_fn: impl Fn() -> Vec<Tensor>)"
    status: implemented
    notes: "Strong Wolfe line search inside step()"

  - contract: cma-es-kernel-v1.yaml
    equation: sample
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::ask"
    signature: "fn ask(&mut self) -> Vec<Vec<f64>>"
    status: implemented
    notes: "Sample population from N(m, sigma^2*C)"

  - contract: cma-es-kernel-v1.yaml
    equation: mean_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Weighted recombination of best mu solutions"

  - contract: cma-es-kernel-v1.yaml
    equation: covariance_update
    module_path: "aprender::metaheuristics::CmaEs"
    function: "CmaEs::tell"
    signature: "fn tell(&mut self, solutions: &[(Vec<f64>, f64)])"
    status: implemented
    notes: "Rank-one and rank-mu covariance update"

  # --- Model-architecture contracts (Qwen showcase + Qwen3 specs) ---

  - contract: model-config-algebra-v1.yaml
    equation: divisibility
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — config constraints tested via proptest strategies (no API needed)"

  - contract: model-config-algebra-v1.yaml
    equation: bounds
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dimension bounds verified algebraically"

  - contract: model-config-algebra-v1.yaml
    equation: cross_constraint
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Float parameter constraints (rope_theta, eps)"

  - contract: qk-norm-v1.yaml
    equation: qk_rmsnorm
    module_path: "aprender::nn::RMSNorm"
    function: "RMSNorm::forward"
    signature: "fn forward(&self, input: &Tensor) -> Tensor"
    status: implemented
    notes: "Per-head QK normalization uses RMSNorm; tested via RMSNorm::without_affine"

  - contract: qk-norm-v1.yaml
    equation: qk_rmsnorm
    module_path: "trueno_gpu::kernels::layernorm::PerHeadRmsNormKernel"
    function: "PerHeadRmsNormKernel::build_ptx"
    signature: "fn build_ptx(&self) -> PtxKernel"
    status: implemented
    notes: >
      GPU per-head RMSNorm using rsqrt.approx and warp shuffle reduction.
      Launched via realizar::per_head_rmsnorm_into(). Cross-backend equivalence
      tested by FALSIFY-QKN-005 (GPU reference simulation vs CPU RMSNorm::forward).

  - contract: tensor-shape-flow-v1.yaml
    equation: qkv_projection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — shape arithmetic tested via proptest (no matmul needed)"

  - contract: tensor-shape-flow-v1.yaml
    equation: gqa_grouping
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "GQA grouping divisibility checked algebraically"

  - contract: tensor-shape-flow-v1.yaml
    equation: swiglu_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "SwiGLU dimension chain verified algebraically"

  - contract: roofline-model-v1.yaml
    equation: bandwidth_ceiling
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — roofline algebra tested inline (no aprender API needed)"

  - contract: roofline-model-v1.yaml
    equation: throughput_bound
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Throughput bounded by min(bw_ceiling, compute_ceiling)"

  - contract: gated-delta-net-v1.yaml
    equation: decay
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_decay"
    signature: "fn gated_delta_net_decay(a_log: &[f32], dt: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Decay in gated_delta_rule_step(): decay = exp(g_h), S *= decay (line 648-652 of linear_attn.rs)"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_write"
    signature: "fn gated_delta_net_write(state: &mut [f32], k: &[f32], delta: &[f32], decay: &[f32])"
    status: implemented
    notes: "State write in gated_delta_rule_step(): S += k ⊗ delta (line 670-680 of linear_attn.rs)"

  - contract: gated-delta-net-v1.yaml
    equation: output
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_output"
    signature: "fn gated_delta_net_output(y: &[f32], z: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Output in gated_delta_rule_step(): o = S^T @ q, then rms_norm_gated (line 682-694 of linear_attn.rs)"

  - contract: format-parity-v1.yaml
    equation: transpose_involution
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — swap(swap(shape)) == shape tested via proptest"

  - contract: format-parity-v1.yaml
    equation: element_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Element count preservation tested algebraically"

  - contract: format-parity-v1.yaml
    equation: identity_1d
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "1D tensor identity mapping tested via proptest"

  # --- Batch 2 contracts (GH-288/290/291/292/293/295/298) ---

  - contract: shannon-entropy-v1.yaml
    equation: entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Shannon entropy tested inline (no aprender API needed)"

  - contract: shannon-entropy-v1.yaml
    equation: uniform_entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Uniform entropy monotonicity tested via proptest"

  - contract: f16-conversion-v1.yaml
    equation: f16_to_f32_bias
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — IEEE 754 bias trick tested inline"

  - contract: f16-conversion-v1.yaml
    equation: roundtrip
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "f16→f32→f16 roundtrip tested for normal bit patterns"

  - contract: kernel-launch-budget-v1.yaml
    equation: per_token_launches
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — kernel count formula tested via proptest"

  - contract: kernel-launch-budget-v1.yaml
    equation: per_layer_decomposition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Static decomposition sum verified (2+5+1+1+1+2=12)"

  - contract: kernel-launch-budget-v1.yaml
    equation: bsum_budget
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Launch count monotonicity tested via proptest"

  - contract: tensor-inventory-v1.yaml
    equation: tensor_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — tensor count formula tested with random configs"

  - contract: tensor-inventory-v1.yaml
    equation: architecture_delta
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Delta between architectures proportional to L"

  - contract: tensor-inventory-v1.yaml
    equation: parameter_decomposition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Tied embeddings reduce count by exactly 1"

  - contract: tensor-inventory-v1.yaml
    equation: quantization_bytes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Quantization size ordering: Q4K < Q6K < Q8 < F16 < F32"

  - contract: performance-grading-v1.yaml
    equation: ollama_parity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — grade boundaries exhaustive and monotonic"

  - contract: performance-grading-v1.yaml
    equation: efficiency_grade
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Efficiency grade monotonic via proptest"

  - contract: performance-grading-v1.yaml
    equation: concrete_instance
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DDR4 33 GB/s, 4.19 GB model => ~7.9 tok/s verified"

  - contract: lora-algebra-v1.yaml
    equation: task_vector
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — task vector roundtrip tested via proptest"

  - contract: lora-algebra-v1.yaml
    equation: eckart_young
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Rank-1 matrix SVD recovery tested (simplified Eckart-Young)"

  - contract: lora-algebra-v1.yaml
    equation: lora_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "A[m,r] @ B[r,n] = [m,n] shape compatibility verified"

  - contract: lora-algebra-v1.yaml
    equation: dare_unbiased
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DARE unbiasedness: (1-p) * 1/(1-p) * delta = delta"

  - contract: lora-algebra-v1.yaml
    equation: shape_preservation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Merge preserves base tensor shapes"

  - contract: quantization-ordering-v1.yaml
    equation: size_ordering
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — strict size ordering tested via proptest"

  - contract: quantization-ordering-v1.yaml
    equation: bytes_per_param
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Concrete Qwen3.5 9B sizes within 20% of expected"

  - contract: quantization-ordering-v1.yaml
    equation: alpha_scaling
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "LoRA alpha/rank scaling verified (16/64=0.25)"

  - contract: quantization-ordering-v1.yaml
    equation: dropout_expectation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "DARE dropout rescale expectation = 1.0"

  # --- Batch 3 contracts (GH-286/287/289/294/296/297/299/300) ---

  - contract: q4k-q6k-superblock-v1.yaml
    equation: q4k_superblock
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Q4K 144 bytes = 2+2+12+128 for 256 elements"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: q6k_superblock
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Q6K 210 bytes = 128+64+16+2 for 256 elements"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: total_bytes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Monotonic total bytes for quantized weight matrix"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: dequantization
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dequant produces finite values, offset vanishing tested"

  - contract: q4k-q6k-superblock-v1.yaml
    equation: bsum
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "bsum depends only on quantized values, not scales"

  - contract: sampling-algorithms-v1.yaml
    equation: greedy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — greedy == argmax tested via proptest"

  - contract: sampling-algorithms-v1.yaml
    equation: top_k
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "At most K nonzero probabilities after filtering"

  - contract: sampling-algorithms-v1.yaml
    equation: top_p
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Cumulative probability >= p for retained tokens"

  - contract: sampling-algorithms-v1.yaml
    equation: temperature
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "T=1 identity: softmax(l/1) == softmax(l)"

  - contract: validated-tensor-v1.yaml
    equation: density_gate
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Density > 0.055 for valid embeddings, rejects sparse"

  - contract: validated-tensor-v1.yaml
    equation: nan_inf_rejection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "NaN and Inf injection detected"

  - contract: validated-tensor-v1.yaml
    equation: l2_norm_nondegeneracy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Zero rows detected, non-zero rows pass"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: hybrid_dispatch
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — exhaustive partition of layer types"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: linear_associativity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Scalar multiplication associativity as proxy for matrix associativity"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: linear_shapes
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Head grouping exact and residual shape preserved"

  - contract: hybrid-layer-dispatch-v1.yaml
    equation: conv1d_causal
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Causal conv1d output length == input length"

  - contract: qwen35-shapes-v1.yaml
    equation: q_projection_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Qwen3.5-9B: 16 * 256 = 4096 (square Q projection)"

  - contract: qwen35-shapes-v1.yaml
    equation: kv_projection_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Qwen3.5-9B: 4 * 256 = 1024, GQA ratio = 4"

  - contract: qwen35-shapes-v1.yaml
    equation: swiglu_ratio
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "12288 / 4096 = 3.0"

  - contract: qwen35-shapes-v1.yaml
    equation: rope_frequency
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "RoPE frequencies strictly decreasing, freq_0 = 1.0"

  - contract: kv-cache-sizing-v1.yaml
    equation: per_token_per_layer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "kv_bytes = 2 * n_kv * d_k * bpe"

  - contract: kv-cache-sizing-v1.yaml
    equation: total_kv_memory
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Monotonic in sequence length and layer count"

  - contract: kv-cache-sizing-v1.yaml
    equation: hybrid_accounting
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Only attention layers contribute to KV cache"

  - contract: kv-cache-sizing-v1.yaml
    equation: bias_absence
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "W @ zeros = zeros when no bias"

  - contract: backend-dispatch-v1.yaml
    equation: gpu_threshold
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dispatch monotonic: larger tensors get higher dispatch level"

  - contract: backend-dispatch-v1.yaml
    equation: garbage_oracle
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Repetitive text detected, diverse text passes"

  - contract: backend-dispatch-v1.yaml
    equation: qk_norm_score_bound
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Dot product of unit vectors bounded by sqrt(d)"

  - contract: kv-cache-equivalence-v1.yaml
    equation: prefill_incremental
    module_path: "realizar::gguf::inference::forward::batch_part_02_part_03"
    function: prefill_batch
    signature: "fn prefill_batch(model: &mut Model, tokens: &[u32], ...) -> Result<Vec<f32>>"
    status: implemented
    notes: "Batched and chunked prefill in realizar; KV cache populated incrementally"

  - contract: kv-cache-equivalence-v1.yaml
    equation: page_shape
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "page_elements = block_size * n_kv * d_k"

  - contract: kv-cache-equivalence-v1.yaml
    equation: batched_serial_equivalence
    module_path: "realizar::gguf::inference::forward"
    function: generate_with_batched_prefill
    signature: "fn generate_with_batched_prefill(...) -> Result<Vec<u32>>"
    status: implemented
    notes: "Batched prefill produces same KV cache as serial; tested via FALSIFY-KV gates"

  - contract: kv-cache-equivalence-v1.yaml
    equation: fused_kernel
    module_path: "realizar::cuda::executor::kv_cache_part_02_part_03"
    function: flash_attention_cached
    signature: "fn flash_attention_cached(q: &CudaSlice, k_cache: &CudaSlice, v_cache: &CudaSlice, ...) -> Result<()>"
    status: implemented
    notes: "CUDA flash attention with fused KV cache read in realizar"

  # --- Qwen 3.5 extended contracts ---

  - contract: sliding-window-attention-v1.yaml
    equation: window_mask
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — window mask algebra tested via proptest"

  - contract: sliding-window-attention-v1.yaml
    equation: causal_window_mask
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — causal constraint tested via proptest"

  - contract: sliding-window-attention-v1.yaml
    equation: effective_context
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — min(i+1, W) formula tested"

  - contract: sliding-window-attention-v1.yaml
    equation: attention_sparsity
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — sparsity asymptote verified"

  - contract: sliding-window-attention-v1.yaml
    equation: multi_layer_receptive_field
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — receptive field linear growth tested"

  - contract: rope-extrapolation-v1.yaml
    equation: base_frequency
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::new"
    signature: "fn new(dim: usize, max_seq_len: usize, base: f32) -> Self"
    status: implemented
    notes: "Base frequency computed in constructor"

  - contract: rope-extrapolation-v1.yaml
    equation: ntk_scaled_base
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Ntk and DynamicNtk in realizar; tested in position_part_04_part_02"

  - contract: rope-extrapolation-v1.yaml
    equation: linear_interpolation
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Linear in realizar; context_multiplier tested"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_ramp
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Yarn with ramp function in realizar; auto_attn_factor tested"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_mixed_frequency
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "YaRN mixed-frequency applied via ScaledRope::apply(); tested in position_part_04_part_02"

  - contract: rope-extrapolation-v1.yaml
    equation: rotation_matrix
    module_path: "aprender::nn::RotaryPositionEmbedding"
    function: "RotaryPositionEmbedding::apply"
    signature: "fn apply(&self, x: &Tensor, position_ids: &[usize]) -> Tensor"
    status: implemented
    notes: "Rotation applied per-head; orthogonality is intrinsic to cos/sin construction"

  - contract: embedding-algebra-v1.yaml
    equation: embedding_lookup
    module_path: "aprender::nn::Embedding"
    function: "Embedding::forward"
    signature: "fn forward(&self, input: &[usize]) -> Tensor"
    status: implemented
    notes: "Standard embedding lookup by token ID"

  - contract: embedding-algebra-v1.yaml
    equation: unembedding_projection
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Uses matmul with transposed embedding weight (lm_head)"

  - contract: embedding-algebra-v1.yaml
    equation: tied_weights
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Weight tying configured in Qwen2 model builder"

  - contract: embedding-algebra-v1.yaml
    equation: vocabulary_bounds
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — bounds checked at tokenization layer"

  - contract: embedding-algebra-v1.yaml
    equation: embedding_norm
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Non-degeneracy tested via proptest on random embeddings"

  - contract: embedding-algebra-v1.yaml
    equation: logit_temperature
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — temperature scaling tested inline"

  - contract: inference-pipeline-v1.yaml
    equation: prefill_phase
    module_path: "realizar::gguf::inference::forward::batch_part_02_part_03"
    function: prefill_batch
    signature: "fn prefill_batch(model: &mut Model, tokens: &[u32], ...) -> Result<Vec<f32>>"
    status: implemented
    notes: "Batched and chunked prefill in realizar; 8.2x speedup over serial"

  - contract: inference-pipeline-v1.yaml
    equation: decode_step
    module_path: "realizar::gguf::inference::forward"
    function: "Model::forward"
    signature: "fn forward(&mut self, token: u32, position: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "Single-token forward pass in realizar; decode is the default path after prefill"

  - contract: inference-pipeline-v1.yaml
    equation: residual_stream
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — residual arithmetic tested via proptest"

  - contract: inference-pipeline-v1.yaml
    equation: layer_composition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — pre-norm + residual composition tested"

  - contract: inference-pipeline-v1.yaml
    equation: hybrid_layer_schedule
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — partition exhaustiveness tested via proptest"

  - contract: inference-pipeline-v1.yaml
    equation: kv_cache_growth
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — linear growth in t tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: attention_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — shape preservation and residual tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gdn_sublayer
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_forward"
    signature: "fn gated_delta_net_forward(input: &[f32], weights: &GdnWeights, state: &mut GdnState) -> Vec<f32>"
    status: implemented
    notes: "forward_linear_block_with_cache() in linear_attn.rs: pre-norm → projections → causal Conv1D → SiLU → gated_delta_rule_step → RMSNorm_gated → out_proj → residual"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: ffn_sublayer
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — SwiGLU shape round-trip tested"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: hybrid_block
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — exclusive OR layer type tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: activation_magnitude
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — RMSNorm bounding tested via proptest"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gradient_flow
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — residual identity Jacobian verified"

  - contract: attention-scaling-v1.yaml
    equation: scaled_dot_product
    module_path: "aprender::nn::transformer"
    function: scaled_dot_product_attention
    signature: "fn scaled_dot_product_attention(query: &Tensor, key: &Tensor, value: &Tensor, attn_mask: Option<&Tensor>, dropout_p: f32, training: bool) -> (Tensor, Tensor)"
    status: implemented
    notes: "1/sqrt(d_k) scaling applied in both aprender SDPA and realizar GQA attention kernels"

  - contract: attention-scaling-v1.yaml
    equation: variance_preservation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — variance ≈ 1 for scaled dot product tested via proptest"

  - contract: attention-scaling-v1.yaml
    equation: softmax_saturation
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — entropy reduction with magnitude tested"

  - contract: attention-scaling-v1.yaml
    equation: score_bound_with_qknorm
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — Cauchy-Schwarz bound on unit-norm vectors"

  - contract: attention-scaling-v1.yaml
    equation: attention_entropy
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — entropy bounds [0, log(m)] tested via proptest"

  - contract: attention-scaling-v1.yaml
    equation: numerical_stability
    module_path: "aprender::nn::functional::softmax"
    function: softmax
    signature: "fn softmax(x: &Tensor, dim: i32) -> Tensor"
    status: implemented
    notes: "Max-subtraction trick implemented in softmax"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: model_parameter_count
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — parameter sum formula tested with Qwen3.5 constants"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: flops_per_token
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — 2P approximation tested via proptest"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: memory_breakdown
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — memory components additive and ordered"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: throughput_model
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — roofline min(bw, compute) tested via proptest"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: verification_ladder
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Meta-contract — coverage fraction tested via pv coverage"

  - contract: qwen35-e2e-verification-v1.yaml
    equation: contract_composition
    module_path: ~
    function: ~
    signature: ~
    status: implemented
    notes: "Pure math — compositional shape preservation tested"

  # --- Core kernel bindings (Tier 1/2) ---

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "realizar::apr::helpers"
    function: simple_attention
    signature: "fn simple_attention(q: &[f32], k: &[f32], v: &[f32], seq_len: usize, num_heads: usize, num_kv_heads: usize, head_dim: usize) -> Vec<f32>"
    status: implemented
    notes: "Multi-sequence causal attention in realizar; GQA-aware with head broadcasting"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "realizar::gguf::inference::attention_part_02"
    function: flash_attention_tiled
    signature: "fn flash_attention_tiled(q: &[f32], k: &[f32], v: &[f32], ...) -> Vec<f32>"
    status: implemented
    notes: "CPU tiled + CUDA flash attention (causal, multi-head, cached) in realizar"

  - contract: gated-delta-net-v1.yaml
    equation: decay
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_decay"
    signature: "fn gated_delta_net_decay(a_log: &[f32], dt: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Decay in gated_delta_rule_step(): decay = exp(g_h), S *= decay (line 648-652)"

  - contract: gated-delta-net-v1.yaml
    equation: read
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_read"
    signature: "fn gated_delta_net_read(state: &[f32], q: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "State read in gated_delta_rule_step(): mem = S^T @ k_norm (line 654-665)"

  - contract: gated-delta-net-v1.yaml
    equation: delta
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_delta"
    signature: "fn gated_delta_net_delta(v: &[f32], read: &[f32], beta: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Delta rule in gated_delta_rule_step(): delta = beta * (v - mem) (line 667-680)"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_write"
    signature: "fn gated_delta_net_write(state: &mut [f32], k: &[f32], delta: &[f32], decay: &[f32])"
    status: implemented
    notes: "State write in gated_delta_rule_step(): S += k ⊗ delta (line 670-680)"

  - contract: gated-delta-net-v1.yaml
    equation: output
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_output"
    signature: "fn gated_delta_net_output(y: &[f32], z: &[f32]) -> Vec<f32>"
    status: implemented
    notes: "Output in gated_delta_rule_step(): o = S^T @ q_norm (line 682-694)"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "realizar::apr::helpers"
    function: simple_attention
    signature: "fn simple_attention(q: &[f32], k: &[f32], v: &[f32], seq_len: usize, num_heads: usize, num_kv_heads: usize, head_dim: usize) -> Vec<f32>"
    status: implemented
    notes: "GQA with head broadcasting (num_heads/num_kv_heads grouping) in realizar"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::silu_standalone"
    function: silu_standalone_scalar
    signature: "fn silu_standalone_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar/AVX2/PTX in provable-contracts; also in realizar::gguf::ops::silu"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "provable_contracts::kernels::silu_standalone"
    function: sigmoid_scalar
    signature: "fn sigmoid_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar/AVX2/PTX; also in trueno backends (6 architectures)"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_discretize
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented; requires zero-order hold discretization (Mamba paper §3.2)"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented; requires parallel associative scan (Mamba paper §3.3)"

  - contract: ssm-kernel-v1.yaml
    equation: selective_gate
    module_path: ~
    function: ~
    signature: ~
    status: not_implemented
    notes: "SSM/Mamba not yet implemented; requires input-dependent selection (Mamba paper §3.4)"

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "provable_contracts::kernels::swiglu"
    function: swiglu_scalar
    signature: "fn swiglu_scalar(gate: &[f32], value: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar/AVX2/PTX; also fused in realizar::quantize::activation_part_02 and CUDA"

  - contract: swiglu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Shared with silu-kernel-v1; SiLU component of SwiGLU"

  # --- Qwen 3.5 binding gap fills ---

  - contract: rope-extrapolation-v1.yaml
    equation: ntk_scaled_base
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Ntk and DynamicNtk in realizar"

  - contract: rope-extrapolation-v1.yaml
    equation: linear_interpolation
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Linear with context_multiplier in realizar"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_ramp
    module_path: "realizar::layers::position"
    function: "ScaledRope::new"
    signature: "fn new(config: RopeConfig) -> Self"
    status: implemented
    notes: "RopeScalingType::Yarn with ramp function in realizar"

  - contract: rope-extrapolation-v1.yaml
    equation: yarn_mixed_frequency
    module_path: "realizar::layers::position"
    function: "ScaledRope::apply"
    signature: "fn apply(&self, x: &[f32], position: usize, head_dim: usize) -> Vec<f32>"
    status: implemented
    notes: "YaRN mixed-frequency via ScaledRope in realizar"

  - contract: qwen35-hybrid-forward-v1.yaml
    equation: gdn_sublayer
    module_path: "realizar::gpu::scheduler::linear_attn"
    function: "gated_delta_net_forward"
    signature: "fn gated_delta_net_forward(input: &[f32], weights: &GdnWeights, state: &mut GdnState) -> Vec<f32>"
    status: implemented
    notes: "Full 216-line impl in realizar::gpu::scheduler::linear_attn::forward_linear_block_incremental — Conv1D, SiLU, delta rule, L2 norm"

  - contract: inference-pipeline-v1.yaml
    equation: prefill_phase
    module_path: "realizar::gguf::inference::forward::batch_part_02_part_03"
    function: prefill_batch
    signature: "fn prefill_batch(model: &mut Model, tokens: &[u32], ...) -> Result<Vec<f32>>"
    status: implemented
    notes: "Batched prefill in realizar; 8.2x speedup over serial"

  - contract: inference-pipeline-v1.yaml
    equation: decode_step
    module_path: "realizar::gguf::inference::forward"
    function: "Model::forward"
    signature: "fn forward(&mut self, token: u32, position: usize) -> Result<Vec<f32>>"
    status: implemented
    notes: "Single-token decode via KV-cached forward pass in realizar"

  - contract: qwen35-shapes-v1.yaml
    equation: o_projection_transpose
    module_path: "realizar::gguf::config"
    function: "ArchConstraints::from_gguf_metadata"
    signature: "fn from_gguf_metadata(metadata: &GgufMetadata) -> ArchConstraints"
    status: implemented
    notes: "O projection shape [hidden, n_h * d_k] validated by ArchConstraints in realizar"

  # --- GH-279: Architecture Requirements (Weight Loading Validation) ---

  - contract: architecture-requirements-v1.yaml
    equation: gguf_loader_validation
    module_path: "realizar::cuda::executor::weights"
    function: "CudaExecutor::build_indexed_weights"
    signature: "fn build_indexed_weights(&mut self, num_layers: usize, layer_prefix_fn: F, arch: &ArchConstraints) -> Result<(), GpuError>"
    status: implemented
    notes: "GGUF loader path — validates via ValidatedLayerWeights::validate()"

  - contract: architecture-requirements-v1.yaml
    equation: apr_loader_validation
    module_path: "realizar::apr::cuda_part_02_part_03"
    function: "AprV2ModelCuda::upload_quantized_weights_to_gpu"
    signature: "fn upload_quantized_weights_to_gpu(&mut self) -> Result<usize>"
    status: implemented
    notes: "APR loader path — derives ArchConstraints from metadata, validates via build_indexed_weights"

  - contract: architecture-requirements-v1.yaml
    equation: safetensors_loader_validation
    module_path: "realizar::safetensors_cuda"
    function: "SafeTensorsCudaModel::upload_weights"
    signature: "fn upload_weights(...) -> Result<(Vec<f32>, ...)>"
    status: implemented
    notes: "SafeTensors loader calls validate_safetensors_completeness() + validate_model_load_basic() before GPU init (GH-279)"

  - contract: architecture-requirements-v1.yaml
    equation: import_completeness_gate
    module_path: "aprender::format::layout_contract"
    function: "enforce_architecture_completeness"
    signature: "fn enforce_architecture_completeness(tensor_names: &[&str], architecture: &str, num_layers: usize) -> Result<(), ContractError>"
    status: implemented
    notes: "Import/export boundary check — rejects incomplete models before writing APR file"

  # --- Kernel reference implementations (provable-contracts::kernels) ---
  # These map each compute kernel contract to its scalar/AVX2/PTX reference
  # implementation in the provable-contracts crate's kernels module.

  - contract: activation-kernel-v1.yaml
    equation: relu
    module_path: "provable_contracts::kernels::activation"
    function: relu_scalar
    signature: "fn relu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: activation-kernel-v1.yaml
    equation: gelu
    module_path: "provable_contracts::kernels::activation"
    function: gelu_scalar
    signature: "fn gelu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: activation-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::activation"
    function: silu_scalar
    signature: "fn silu_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: silu-kernel-v1.yaml
    equation: silu
    module_path: "provable_contracts::kernels::silu_standalone"
    function: silu_standalone_scalar
    signature: "fn silu_standalone_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: silu-kernel-v1.yaml
    equation: sigmoid
    module_path: "provable_contracts::kernels::silu_standalone"
    function: sigmoid_scalar
    signature: "fn sigmoid_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: softmax-kernel-v1.yaml
    equation: softmax
    module_path: "provable_contracts::kernels::softmax"
    function: softmax_scalar
    signature: "fn softmax_scalar(input: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: rmsnorm-kernel-v1.yaml
    equation: rmsnorm
    module_path: "provable_contracts::kernels::rmsnorm"
    function: rmsnorm_scalar
    signature: "fn rmsnorm_scalar(input: &[f32], gamma: &[f32], eps: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: layernorm-kernel-v1.yaml
    equation: layernorm
    module_path: "provable_contracts::kernels::layernorm"
    function: layernorm_scalar
    signature: "fn layernorm_scalar(input: &[f32], gamma: &[f32], beta: &[f32], eps: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: batchnorm-kernel-v1.yaml
    equation: batchnorm_train
    module_path: "provable_contracts::kernels::batchnorm"
    function: batchnorm_scalar
    signature: "fn batchnorm_scalar(input: &[f32], n: usize, c: usize, gamma: &[f32], beta: &[f32], eps: f32, output: &mut [f32], mean: &mut [f32], var: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: swiglu-kernel-v1.yaml
    equation: swiglu
    module_path: "provable_contracts::kernels::swiglu"
    function: swiglu_scalar
    signature: "fn swiglu_scalar(gate: &[f32], value: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: cross-entropy-kernel-v1.yaml
    equation: cross_entropy
    module_path: "provable_contracts::kernels::cross_entropy"
    function: cross_entropy_scalar
    signature: "fn cross_entropy_scalar(targets: &[f32], logits: &[f32], num_classes: usize) -> f32"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: rope-kernel-v1.yaml
    equation: rope
    module_path: "provable_contracts::kernels::rope"
    function: rope_scalar
    signature: "fn rope_scalar(x: &[f32], position: usize, dim: usize, base: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: matmul-kernel-v1.yaml
    equation: matmul
    module_path: "provable_contracts::kernels::matmul"
    function: matmul_scalar
    signature: "fn matmul_scalar(a: &[f32], b: &[f32], m: usize, p: usize, n: usize, c: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: attention-kernel-v1.yaml
    equation: attention
    module_path: "provable_contracts::kernels::attention"
    function: attention_scalar
    signature: "fn attention_scalar(q: &[f32], k: &[f32], v: &[f32], n: usize, m: usize, d_k: usize, d_v: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: gqa-kernel-v1.yaml
    equation: gqa
    module_path: "provable_contracts::kernels::gqa"
    function: gqa_scalar
    signature: "fn gqa_scalar(q: &[f32], k: &[f32], v: &[f32], n: usize, d_k: usize, d_v: usize, num_heads: usize, num_kv_heads: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: flash-attention-v1.yaml
    equation: flash_attention
    module_path: "provable_contracts::kernels::flash_attention"
    function: flash_attention_scalar
    signature: "fn flash_attention_scalar(q: &[f32], k: &[f32], v: &[f32], n: usize, d: usize, tile_size: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: adamw-kernel-v1.yaml
    equation: weight_update
    module_path: "provable_contracts::kernels::adamw"
    function: adamw_step_scalar
    signature: "fn adamw_step_scalar(params: &mut [f32], grads: &[f32], m: &mut [f32], v: &mut [f32], lr: f32, beta1: f32, beta2: f32, eps: f32, wd: f32, t: u32)"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: conv1d-kernel-v1.yaml
    equation: conv1d
    module_path: "provable_contracts::kernels::conv1d"
    function: conv1d_scalar
    signature: "fn conv1d_scalar(input: &[f32], weight: &[f32], bias: Option<&[f32]>, c_in: usize, c_out: usize, l: usize, k: usize, stride: usize, pad: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: ssm-kernel-v1.yaml
    equation: ssm_scan
    module_path: "provable_contracts::kernels::ssm"
    function: ssm_scan_scalar
    signature: "fn ssm_scan_scalar(a_bar: &[f32], b_bar: &[f32], c: &[f32], x: &[f32], n: usize, l: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: kmeans-kernel-v1.yaml
    equation: assignment
    module_path: "provable_contracts::kernels::kmeans"
    function: kmeans_assign_scalar
    signature: "fn kmeans_assign_scalar(points: &[f32], centroids: &[f32], n: usize, k: usize, d: usize, assignments: &mut [u32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: pagerank-kernel-v1.yaml
    equation: pagerank
    module_path: "provable_contracts::kernels::pagerank"
    function: pagerank_iterate_scalar
    signature: "fn pagerank_iterate_scalar(transition: &[f32], rank: &[f32], n: usize, damping: f32, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: lbfgs-kernel-v1.yaml
    equation: two_loop_recursion
    module_path: "provable_contracts::kernels::lbfgs"
    function: lbfgs_direction_scalar
    signature: "fn lbfgs_direction_scalar(gradient: &[f32], s_history: &[Vec<f32>], y_history: &[Vec<f32>], direction: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: cma-es-kernel-v1.yaml
    equation: sample
    module_path: "provable_contracts::kernels::cma_es"
    function: cma_sample_scalar
    signature: "fn cma_sample_scalar(mean: &[f32], sigma: f32, cholesky_l: &[f32], d: usize, z: &[f32], output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"

  - contract: gated-delta-net-v1.yaml
    equation: write
    module_path: "provable_contracts::kernels::gated_delta_net"
    function: gdn_recurrence_scalar
    signature: "fn gdn_recurrence_scalar(state: &mut [f32], k: &[f32], v: &[f32], q: &[f32], z: &[f32], alpha: f32, beta: f32, k_dim: usize, v_dim: usize, output: &mut [f32])"
    status: implemented
    notes: "Scalar + AVX2 + PTX backends"
